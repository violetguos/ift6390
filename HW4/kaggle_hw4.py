# -*- coding: utf-8 -*-
"""kaggle_hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RbrkrSU66ZSP2ulBKRLB4UvZBphdKdjU

# Kaggle testing
"""

!pip install -U -q PyDrive

# import data from Google drive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# software, math libraries
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
from sklearn.svm import SVC
import pandas as pd

# 1. Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# 2. Import data from google drive (id is from google drive shareable link)

# Get training data
download = drive.CreateFile({'id': '1BEp0SzrCinaDP3Icb8PkTQge5c1ymS94'})
download.GetContentFile('train_images.npy')
download = drive.CreateFile({'id': '1pKyWJtNEu3O1XukE75iOHteLHRVCDjPG'})
download.GetContentFile('train_labels.csv')

# Get test data
download = drive.CreateFile({'id': '1BEp0SzrCinaDP3Icb8PkTQge5c1ymS94'})
download.GetContentFile('test_images.npy')

"""Let's visualize the data with one from each class."""

'''# Load the training data
training_images = np.load('train_images.npy', encoding='latin1')

# Load the testing data
test_images = np.load('test_images.npy', encoding='latin1')'''

'''
read everything as 3 columns, ID, IMG, CATEGORY

Id                                                img  Category  \
0  0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...    shovel   
1  1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...     rifle 

Category_num  
0             0  
1             1 

'''


#! CHANGE THIS!!

images_train = np.load('./train_images.npy', encoding='latin1')
train_labels = pd.read_csv('./train_labels.csv')

label_list = train_labels.Category.unique().tolist()
label_dict = {label:i  for i,label in enumerate(label_list)}
train_labels['Category_num'] = train_labels.Category.apply(lambda x: label_dict[x])

train_df = pd.DataFrame(images_train, columns = ['Id', 'img'])
train_df = pd.merge(train_df, train_labels, on = 'Id')

images_test = np.load('./test_images.npy', encoding='latin1')
test_df = pd.DataFrame(images_test, columns = ['Id', 'img'])

#print(train_df.head(n=5))

training_vectors = np.array(train_df['img'].tolist())
testing_vectors = np.array(test_df['img'].tolist())
training_labels = train_labels['Category_num']

print(training_vectors.shape)

# Get vectors and their labels
#training_vectors = training_images[:,-1]
#test_vectors = test_images[:,-1]


'''with open('train_labels.csv') as fp:
  label_lines = fp.readlines()
  training_labels = []
  for line in label_lines[1:]:
    training_labels.append(line.split(',')[1].strip())

training_labels = np.array(training_labels)'''



'''# Get first example of each class in the training data
example_labels = list(set(training_labels))
example_indices = [list(training_labels).index(x) for x in example_labels]
example_images = [training_vectors[x] for x in example_indices]

# Plot
fig, axs = plt.subplots(6,6, figsize=(20, 15))
fig.subplots_adjust(hspace = .5, wspace=0)

axs = axs.ravel()

for i, ax in enumerate(axs):
    if i < len(example_images):
      axs[i].imshow(example_images[i].reshape(100, 100))
      axs[i].set_title(example_labels[i])'''

# Preprocessing - Erosion
import cv2

I = 14
image_train1 = (images_train[I][1]).reshape(100,100)
plt.figure()
plt.imshow(image_train1,cmap='gray')


#Printing label
print(train_labels.iloc[I].values[1])
#print(train_labels.iloc[I])



kernel = np.ones((2,2),np.uint8)
erosion = cv2.erode(image_train1,kernel,iterations = 1)
plt.figure()
plt.imshow(erosion)

# Preprocessing - dilation

kernel2 = np.ones((1,1),np.uint8)
dilation = cv2.dilate(erosion,kernel2,iterations = 1)
plt.figure()
plt.imshow(dilation)

# Pre processing 
plt.figure()
plt.imshow(image_train1)
#kernelhm = np.ones((1,1),np.uint8)
kernelhm = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(2,2))
openning = cv2.morphologyEx(image_train1, cv2.MORPH_OPEN, kernel)
plt.figure()
plt.imshow(openning)

"""## Baseline SVM

Let's use an SVM classifier to get a baseline rate to beat.
"""

from sklearn.metrics import classification_report
clf = SVC(gamma='auto')

# Train the SVM

print(training_vectors[:1000].shape)
print(training_labels[:1000].shape)
clf.fit(training_vectors[:1000], training_labels[:1000])

predicted = clf.predict(training_vectors[:1000])
# Test the test data
print(clf.score(training_vectors[:1000], training_labels[:1000]))
print(classification_report(training_labels[:1000], predicted[:1000]))

print(clf.score(training_vectors, training_labels))

#simple testing SVM with dummy data
import numpy as np
X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])
print(X.shape)
y = np.array([1, 1, 2, 2])
from sklearn.svm import SVC
clf = SVC(gamma='auto')
clf.fit(X, y) 




print(clf.predict([[-0.8, -1]]))

# http://pytorch.org/
from os.path import exists
from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag
platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())
cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\.\([0-9]*\)\.\([0-9]*\)$/cu\1\2/'
accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'

!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision

import os
import pickle
import random
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import matplotlib.pyplot as plt
# %matplotlib inline

print(f"Your version of Pytorch is {torch.__version__}. You should use a version >0.4.")

import torch.utils.data as utils
from torch.utils.data.sampler import SubsetRandomSampler

# data a list of numpy arrays
# target another list of numpy arrays (targets)

train_data_tensor = torch.stack([torch.Tensor(i) for i in training_vectors]) # transform to torch tensors
train_label_tensor = torch.Tensor(training_labels)

train_dataset = utils.TensorDataset(train_data_tensor, train_label_tensor) # create your datset



# If a GPU is available, use it
# Pytorch uses an elegant way to keep the code device agnostic
if torch.cuda.is_available():
    device = torch.device("cuda")
    use_cuda = True
else:
    device = torch.device("cpu")
    use_cuda = False
    
print(device)

# This parameter influences optimization
batch_size = 64
# This is just for evaluation, we want is as big as the GPU can support
batch_size_eval = 512


indices = list(range(len(training_labels)))
random.shuffle(indices)


# the number for doing train test valid split
# alternatively, we can try to do a random split
# or add K-fold later
# will keep this for now
n_valid = 64

train_loader = utils.DataLoader(
    train_dataset,
    batch_size=batch_size,
    sampler=SubsetRandomSampler(indices[n_valid:]),
    #num_workers=1,
    pin_memory=use_cuda
)

for inputs, targets in train_loader:
    print("shape inputs", inputs.shape)
    plt_inputs = inputs.reshape(64, 100, 100)
    print(f"This is the shape of one batch {inputs.shape}. What is the meaning of each dimension?  batch size * channels * height * width")
    print(f"target", targets.shape, set(targets))
    img = plt_inputs[0]
    print("img shape", img.shape)
    plt.imshow(img, cmap='Greys_r')
    break