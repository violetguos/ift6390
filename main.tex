\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts, physics,bbold, bm}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
 
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
%If you want to title your bold things something different just make another thing exactly like this but replace "problem" with the name of the thing you want, like theorem or lemma or whatever



\begin{document}
 
%\renewcommand{\qedsymbol}{\filledbox}
%Good resources for looking up how to do stuff:
%Binary operators: http://www.access2science.com/latex/Binary.html
%General help: http://en.wikibooks.org/wiki/LaTeX/Mathematics
%Or just google stuff
 
\title{IFT 6390 Homework 1}
\author{Author}
\maketitle
 
\begin{problem}{1}
Small exercise on probabilities
\end{problem}
 
\begin{proof}
p
\end{proof}

%%%%%%%% part 3.1 %%%%%%%%
\begin{problem}{3.1 (a)}
Name parameters
\end{problem}

\begin{proof}
the parameters are $\sigma$ and $\mu$\\
$\mu \in (d , 1)$ \\
The matrix $\Sigma$ is d by d, where the diagonal terms are $\sigma$
\end{proof}

\begin{problem}{3.1 (b)}
Equation for optimal parameters
\end{problem}

\begin{proof}
\begin{equation*}
\mu = \frac{1}{n} \sum_{i=1}^{n} x_i
\end{equation*}

\begin{equation*}
\sigma = \frac{1}{n} \sum_{i=1}^{n} (x_i - \mu) (x_i - \mu)^T 
\end{equation*}
\end{proof}

\begin{problem}{3.1 (c)}
What is the algorithmic complexity of this training method, i.e. of the method calculating these parameters?
\end{problem}

\begin{proof}
% will make pseudo code prettier
To computer $\mu$: \\
for i $=$ 1 to n \\
sum each $x_i$ with $d$ components\\
endfor\\
Total is $O(nd)$

To computer $\sigma$: \\
for i $=$ 1 to n \\
 subtract $\mu$ from each $x_i$ with $d$ components\\
 repeat the last operation with transpose
 multiply $d$ by $1$ and  $1$ by $d$ vectors, result in a $d$ by $d$ matrix, which is $d^2$ operations
endfor\\
Total is $O(nd^2 + d + d ) = O(nd^2)$

\end{proof}


%%%%%%%%%%%part 3.2%%%%%%%%%%%%%
\begin{problem}{3.2(a)}
Suppose that the user has fixed $\sigma$ What does the "training/learning" phase of these Parzen windows consist of?
\end{problem}
\begin{proof}
Center at each $x$, calculate normal density function with $\sigma$
\end{proof}

\begin{problem}{3.2(b)}
For a test point x, write in a single detailed formula (i.e. with exponentials), the function that will give the probability density predicted at point $x$: $p_{parzen}(x) = $?
\end{problem}
\begin{proof}
\begin{equation*}
\hat{p}_{Parzen}(x) = \sum_{i = 1}^{n} \frac{1}{(2\pi \sigma^2)^{d/2}} exp(\frac{1}{2} \frac{\abs{x - \mu}^2}{\sigma})
\end{equation*}
\end{proof}

\begin{problem}{3.2(c)}
 What is the algorithmic complexity for calculating this prediction at each new point x?
\end{problem}

\begin{proof}
$O(N)$, since there are n terms and all have fixed $\sigma$ and $\mu$
\end{proof}

%%%%%%%%%%%%%%%%%part 3.3%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{3.3(a)}
Which one of these two approaches (parametric Gaussian v.s. Parzen Gaussian kernel) has the highest capacity (in other words, higher expressivity)? Explain
\end{problem}
\begin{proof}
Parzen
\end{proof}


\begin{problem}{3.3(b)}
With which one of these approaches, and in which scenario, are we likely to be over-fitting (i.e. memorizing the noise in our data)?
\end{problem}
\begin{proof}
Parzen
\end{proof}

\begin{problem}{3.3(c)}
Hyperparam vs param
\end{problem}

\begin{proof}
to do
\end{proof}


%%%%%%%% part 3.4 %%%%%%%%
\begin{problem}{3.4 (a)}
Express the equation of a diagonal Gaussian density in Rd. Specify what are its parameters and their dimensions.
\end{problem}

\begin{proof}
\begin{equation*}
\prod_{i=1}^{D} (2\pi \sigma_{i})^{\frac{1}{2}} exp\{{- \sum_{i=1}^{D}} \frac{1}{2 \sigma_{i} ^2} (x_{i} - \mu_{i})^2 \} 
\end{equation*}
\end{proof}


\begin{problem}{3.4 (b)}
Show that the components of a random vector following a diagonal Gaussian distribution are independent random variables.
\end{problem}

\begin{proof}
First, let us walk through the idea. We would like to prove the independence by proving that the product of each normal distribution's density function made up of the each component of the random vector is equal to the diagonal Gaussian distribution's density function, namely the random vector's density distribution. \\
The Gaussian density function for dataset $D$ is defined as \\

\begin{equation*}
%\begin{split}
p(\bm{x}) = \mathcal{N}_{\mu, \Sigma}(\bm{x}) = \frac{1}{(2\pi)^{d/2}\sqrt{det(\Sigma)}} exp(-\frac{1}{2}(\bm{x} - \mu)^T \Sigma^{-1} (\bm{x} - \mu)) 
\end{equation*}

Since we have a diagonal $\Sigma$, it can be written in the form of a summation of the diagonal terms in the power, and then transformed into a product of exponents. Note that $\sigma_{ii}$ here refers to the diagonal terms.

\begin{equation*}
p(\bm{x}) = \frac{1}{(2\pi)^{d/2}\sqrt{det(\Sigma)}} exp(\sum_{i=1}^{d} -\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) 
\end{equation*}

\begin{equation*}
p(\bm{x}) = \frac{1}{(2\pi)^{d/2}\sqrt{det(\Sigma)}} \prod_{i=1}^{d} exp -(\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2)
\end{equation*}

we can also express the constant term in a product form

\begin{equation*}
\frac{1}{(2\pi)^{d/2}\sqrt{det(\Sigma)}}  = \prod_{i=1}^{d} \frac{1}{(2\pi)^{1/2} \sigma_{ii} } =  \prod_{i=1}^{d} \frac{1}{(2\pi \sigma_{ii}^2)^{1/2} }
\end{equation*}

also, note that the determinant of a diagonal matrix is the product of the diagonal terms

\begin{equation*}
det(\Sigma) = \prod_{i=1}^{d} \sigma_{ii}^2
\end{equation*}

Now, we have the expression for $p(\bm{x})$
\begin{equation*}
p(\bm{x}) = \prod_{i=1}^{d} \frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } \prod_{i=1}^{d} exp (-\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) = \prod_{i=1}^{d} \frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } exp (-\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) 
\end{equation*}

Note that $\frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } exp (\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2)$ is a Gaussian of $\mathcal{N}(\sigma_{ii}, \mu)$

Therefore, we have

\begin{equation*}
p(\bm{x}) = \prod_{i =1}^{d} \mathcal{N}(\sigma_{ii}, \mu)
\end{equation*}

\end{proof}

\begin{problem}{3.4(c)}
Using ? log p(x) as the loss, write down the equation correspond- ing to the empirical risk minimization on the training set D
\end{problem}
\begin{proof}
From part b, we have
\begin{equation*}
p(\bm{x}) = \prod_{i=1}^{d} \frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } exp (\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) 
\end{equation*}

\begin{equation*}
-log p(X) = -log (\prod_{i=1}^{d} \frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } exp (-\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) )
\end{equation*}
\begin{equation*}
-log p(X) =\sum_{i=1}^{d}  -log (\frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } exp (-\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) )
\end{equation*}

\begin{equation*}
-log p(X) = \sum_{i=1}^{d}  -log (\frac{1}{(2\pi \sigma_{ii}^2)^{1/2} } - \sum_{i=1}^{d} log(exp( -(\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) ))
\end{equation*}

\begin{equation*}
-log p(X) = \sum_{i=1}^{d} log ((2\pi \sigma_{ii}^2)^{1/2}  + \sum_{i=1}^{d}  (\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) )
\end{equation*}

\begin{equation*}
\hat{R}(f, D) = \frac{1}{\abs{D}} \sum_{(x , y)\in D} L(f(x), y)
\end{equation*}
\begin{equation*}
\hat{R}(f, D) = \frac{1}{\abs{D}} \sum_{(x , y)\in D} -log(p(X))
\end{equation*}
\begin{equation*}
\hat{R}(f, D) =\frac{1}{\abs{D}} \sum_{(x , y)\in D} \sum_{i=1}^{d} log ((2\pi \sigma_{ii}^2)^{1/2}  + \sum_{i=1}^{d}  (\frac{1}{2 \sigma_{ii}^2}(x_i - \mu)^2) )
\end{equation*}

\end{proof}

\end{document}