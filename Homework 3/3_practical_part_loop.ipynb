{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Part: Neural Network Implementation & Experiments\n",
    "\n",
    "Team:\n",
    "* Jonathan Bhimani-Burrows ()\n",
    "* Arlie Coles (20121051)\n",
    "* Yue (Violet) Guo (20120727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Fashion MNIST data:\n",
    "Note: keep your file structures like this for reading input data without\n",
    "using ```import os``` for path change!\n",
    "```\n",
    "./Homework 3\n",
    "├── 3_practical_part.ipynb\n",
    "├── circles.txt\n",
    "├── data\n",
    "│   ├── fashion\n",
    "│   │   ├── t10k-images-idx3-ubyte.gz\n",
    "│   │   ├── t10k-labels-idx1-ubyte.gz\n",
    "│   │   ├── train-images-idx3-ubyte.gz\n",
    "│   │   └── train-labels-idx1-ubyte.gz\n",
    "│   └── mnist\n",
    "│       └── README.md\n",
    "├── hw3\n",
    "│   └── d3english.pdf\n",
    "├── overleaf_url.txt\n",
    "└── utils\n",
    "    ├── __init__.py\n",
    "    ├── __pycache__\n",
    "    │   ├── __init__.cpython-36.pyc\n",
    "    │   └── mnist_reader.cpython-36.pyc\n",
    "    ├── argparser.py\n",
    "    ├── helper.py\n",
    "    └── mnist_reader.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "import numpy as np\n",
    "import math\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Circles data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 2)\n",
      "[1 1 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "circlesData = np.loadtxt(open('circles.txt','r'))\n",
    "circlesTarget = circlesData[:,2]\n",
    "circlesData = circlesData[:,[0,1]] \n",
    "\n",
    "print(circlesData.shape)\n",
    "circlesTarget = np.array([int(i) for i in circlesTarget])\n",
    "print(circlesTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 2) (110, 2) (110, 2)\n",
      "(880, 2)\n"
     ]
    }
   ],
   "source": [
    "class loadData:\n",
    "    def __init__(self):\n",
    "        self.addOnes = False\n",
    "        self.data_path = '/data/'\n",
    "    \n",
    "    def convertTarget(self, targetValues):\n",
    "        # Convert to one-hot encoding\n",
    "        numClasses = np.max(targetValues) + 1\n",
    "        return np.eye(numClasses)[targetValues]\n",
    "\n",
    "    def loadNumData(self, data, target):\n",
    "        # Split into train/validation/test\n",
    "        np.random.seed(6390)\n",
    "        randIndices = np.random.permutation(data.shape[0])\n",
    "        data, target = data[randIndices], target[randIndices]\n",
    "        \n",
    "        div1 = int(math.floor(0.8 * data.shape[0]))\n",
    "        div2 = int(math.floor(0.9 * data.shape[0]))\n",
    "        trainData, trainTarget = data[:div1], target[:div1]\n",
    "        validData, validTarget = data[div1:div2], target[div1:div2]\n",
    "        testData, testTarget = data[div2:], target[div2:]\n",
    "    \n",
    "        # Get one hot encoding\n",
    "        trainTarget = self.convertTarget(trainTarget)\n",
    "        validTarget = self.convertTarget(validTarget)\n",
    "        testTarget = self.convertTarget(testTarget)\n",
    "        \n",
    "        return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "dataLoader = loadData()\n",
    " \n",
    "trainData, trainTarget, validData, validTarget, testData, testTarget = dataLoader.loadNumData(circlesData, circlesTarget)\n",
    "print(trainData.shape, validData.shape, testData.shape)\n",
    "\n",
    "print(trainTarget.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "> As a beginning, start with an implementation that computes the gradients for a single example, and check that the gradient is correct using the finite difference method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def random_batch_indices(self, m=None):\n",
    "        if m is None:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(self.indices, m, replace=False)\n",
    "        return indices \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        '''\n",
    "        Get a random batch without replacement from the dataset.\n",
    "        If m is given the batch will be of size m. \n",
    "        Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        indices = self.random_batch_indices(m)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our own activation functions\n",
    "\n",
    "def relu(pre_activation):\n",
    "    '''\n",
    "    preactivation is a vector\n",
    "    '''\n",
    "    relu_output = np.zeros(pre_activation.shape)\n",
    "    relu_flat = relu_output.flatten()\n",
    "    for i, neuron in enumerate(pre_activation.flatten()):\n",
    "        if neuron > 0:\n",
    "            relu_flat[i] = neuron\n",
    "    relu_output = relu_flat.reshape(pre_activation.shape)\n",
    "    return relu_output\n",
    "\n",
    "def softmax(pre_activation):\n",
    "    '''\n",
    "    Numerically stable because subtracting the max value makes bit overflow impossible,\n",
    "    we will only have non-positive values in the vector\n",
    "    '''\n",
    "    exps = np.exp(pre_activation - np.max(pre_activation))\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_fixed = np.array([[ 0.20960823 , 0.13663559], [ 0.38623373, -0.32807251] ,[-0.63849439 , 0.0131154 ],\n",
    "                     [ 0.5142807 ,  0.0595152 ], [-0.31075243 , 0.52335846]])\n",
    "w2_fixed = np.array( [[ 0.06159592, -0.10424877,  0.23591191 , 0.06177611 , 0.42799154],\n",
    "                      [ 0.40780062,  0.0759027  , 0.09284926, -0.14837115 ,  0.16844463]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neuralNet():\n",
    "    def __init__(self, d, dh, m, n, eta=1, regularize=None, fixed=False):\n",
    "        self.inputDim = d #inputDim\n",
    "        self.hiddenDim = dh #hiddenDim\n",
    "        self.outputDim = m #outputDim\n",
    "        self.regularize = regularize # lambda value\n",
    "        self.learningRate = eta\n",
    "        self.numData = n\n",
    "        self.batchErrorGradients = []\n",
    "        \n",
    "        # Initial weights and biases\n",
    "        if fixed:\n",
    "            self.W_1 = w1_fixed\n",
    "            self.W_2 = w2_fixed\n",
    "        else:   \n",
    "            self.W_1 = np.random.uniform(-1/np.sqrt(d), 1/np.sqrt(d), d*dh).reshape(dh, d)\n",
    "            self.W_2 = np.random.uniform(-1/np.sqrt(dh), 1/np.sqrt(dh), dh*m).reshape(m, dh) \n",
    "            \n",
    "        self.b_1 = np.zeros(dh).reshape(dh,)\n",
    "        self.b_2 = np.zeros(m).reshape(m,)\n",
    "\n",
    "        \n",
    "    def fprop(self, batchData, batchTarget):\n",
    "        self.h_a = np.dot(self.W_1, batchData.T) + self.b_1\n",
    "        self.h_s = relu(self.h_a)\n",
    "        self.o_a = np.dot(self.W_2, self.h_s) + self.b_2\n",
    "        self.o_s = softmax(self.o_a)\n",
    "\n",
    "    def errorRate(self, y, mode='matrix'):\n",
    "        '''\n",
    "        negative log\n",
    "        -logO_s(x)\n",
    "        '''        \n",
    "        \n",
    "        if mode == 'loop':\n",
    "            negLog = -self.o_a[np.argmax(y)] + np.log(np.sum(np.exp(self.o_a), axis=0))\n",
    "            \n",
    "        elif mode == 'matrix':\n",
    "            negLog = []\n",
    "            for i in range(self.numData):\n",
    "                error_at_point = -self.o_a[np.argmax(y[:,i])][i] + np.log(np.sum(np.exp(self.o_a), axis=0))[i]\n",
    "                negLog.append(error_at_point)\n",
    "            negLog = np.array(negLog)\n",
    "            negLog = np.mean(negLog)\n",
    "\n",
    "        return negLog\n",
    "          \n",
    "    def bpropLoop(self, batchData, batchTarget):\n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        self.grad_W2 = np.outer(self.grad_oa, self.h_s.T)\n",
    "        self.grad_b2 = self.grad_oa\n",
    "        self.grad_hs = np.dot(self.W_2.T , self.grad_oa)\n",
    "        h_a_stack = np.where(self.h_a > 0, 1, 0)\n",
    "        self.grad_ha = np.multiply(self.grad_hs, h_a_stack)\n",
    "        self.grad_W1 = np.outer(self.grad_ha, batchData)\n",
    "        self.grad_b1 = self.grad_ha\n",
    "        \n",
    "    def bprop(self, batchData, batchTarget):\n",
    "        '''\n",
    "        batchTarget already in one-hot format\n",
    "        '''\n",
    "    \n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        self.grad_W2 = [np.outer(self.grad_oa[:,i], self.h_s[:,i].T) for i in range(self.numData)]\n",
    "        self.grad_b2 = self.grad_oa #np.mean(self.grad_oa, axis = 1)        \n",
    "        self.grad_hs = np.dot(self.W_2.T , self.grad_oa)\n",
    "        h_a_stack = np.where(self.h_a > 0, 1, 0)\n",
    "        self.grad_ha = np.multiply(self.grad_hs, h_a_stack)\n",
    "        self.grad_W1 = [np.outer(self.grad_ha[:,i], batchData[i]) for i in range(self.numData)]\n",
    "        self.grad_b1 = self.grad_ha\n",
    "\n",
    "    def updateParams(self):\n",
    "        self.W_1 -= self.grad_W1 * self.learningRate\n",
    "        self.W_2 -= self.grad_W2 * self.learningRate\n",
    "        self.b_1 -= self.grad_b1 * self.learningRate\n",
    "        self.b_2 -= self.grad_b2 * self.learningRate\n",
    "    \n",
    "    def gradDescentLoop(self, batchData, batchTarget, K):\n",
    "        # Call each example in the data (over the minibatches) in a loop\n",
    "        grad_W2, grad_b2, grad_W1, grad_b1 = [], [], [], []\n",
    "        for i in range(K):\n",
    "            self.fprop(batchData[i], batchTarget[:,i]) #batchTarget[:,i]\n",
    "            self.bpropLoop(batchData[i],batchTarget[:,i])\n",
    "            grad_W2.append(self.grad_W2)\n",
    "            grad_b2.append(self.grad_b2)\n",
    "            grad_W1.append(self.grad_W1)\n",
    "            grad_b1.append(self.grad_b1)\n",
    "        self.grad_W2 = np.mean(grad_W2, axis=0)\n",
    "        self.grad_b2 = np.mean(grad_b2, axis=0)\n",
    "        self.grad_W1 = np.mean(grad_W1, axis=0)\n",
    "        self.grad_b1 = np.mean(grad_b1, axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the gradients for a simple network with a single example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient for W2:\n",
      " [[ 0.         -0.03056778]\n",
      " [ 0.          0.03056778]]\n",
      "Gradient for b2:\n",
      " [-0.49642669  0.49642669]\n",
      "Gradient for W1:\n",
      " [[ 0.          0.        ]\n",
      " [-0.09549319  0.06449919]]\n",
      "Gradient for b1:\n",
      " [ 0.         -0.11523496]\n"
     ]
    }
   ],
   "source": [
    "x_i = trainData[0:1]\n",
    "y_i = trainTarget[0:1].T\n",
    "\n",
    "nn = neuralNet(2, 2, 2, 1)\n",
    "nn.gradDescentLoop(x_i, y_i, 1)\n",
    "\n",
    "print('Gradient for W2:\\n', nn.grad_W2)\n",
    "print('Gradient for b2:\\n', nn.grad_b2)\n",
    "print('Gradient for W1:\\n', nn.grad_W1)\n",
    "print('Gradient for b1:\\n', nn.grad_b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we verify these gradients by finite difference estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perturbing an element in W2. Ratio: [0.99999983]\n",
      "Perturbing an element in W2. Ratio: [0.99999849]\n",
      "Perturbing an element in W2. Ratio: [1.00000017]\n",
      "Perturbing an element in W2. Ratio: [1.00000151]\n",
      "Perturbing an element in b2. Ratio: [0.99999785]\n",
      "Perturbing an element in b2. Ratio: [1.00000215]\n",
      "Perturbing an element in W1. Ratio: [1.00000091]\n",
      "Perturbing an element in W1. Ratio: [0.99999938]\n",
      "Perturbing an element in W1. Ratio: [1.00000061]\n",
      "Perturbing an element in W1. Ratio: [0.99999959]\n",
      "Perturbing an element in b1. Ratio: [1.0000011]\n",
      "Perturbing an element in b1. Ratio: [1.00000074]\n"
     ]
    }
   ],
   "source": [
    "# Note to TAs: we attempted this with a loop, but deep/shallow copying issues got in the way.\n",
    "x_i = trainData[0:1]\n",
    "y_i = trainTarget[0:1].T\n",
    "sigma = 1e-5\n",
    "nn = neuralNet(2, 2, 2, 1)\n",
    "\n",
    "# Perturbing W2\n",
    "# 1 of 4 elements to perturb\n",
    "nn.gradDescentLoop(x_i, y_i, 1)\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[0][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][0]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# 2 of 4 elements to perturb\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[0][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][1]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# 3 of 4 elements to perturb\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[1][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[1][0]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# 4 of 4 elements to perturb\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[1][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[1][1]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# Perturbing b2\n",
    "# 1 of 2 elements to perturb\n",
    "grad_b2 = nn.grad_b2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_2[0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b2[0]\n",
    "print('Perturbing an element in b2. Ratio:', ratio)\n",
    "\n",
    "# 2 of 2 elements to perturb\n",
    "grad_b2 = nn.grad_b2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_2[1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b2[1]\n",
    "print('Perturbing an element in b2. Ratio:', ratio)\n",
    "\n",
    "# Perturbing W1\n",
    "# 1 of 4 elements to perturb\n",
    "nn.gradDescentLoop(x_i, y_i, 1)\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[0][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[0][0]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# 2 of 4 elements to perturb\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[0][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[0][1]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# 3 of 4 elements to perturb\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[1][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[1][0]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# 4 of 4 elements to perturb\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[1][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[1][1]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# Perturbing b1\n",
    "# 1 of 2 elements to perturb\n",
    "grad_b1 = nn.grad_b1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_1[0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b1[0]\n",
    "print('Perturbing an element in b1. Ratio:', ratio)\n",
    "\n",
    "# 2 of 2 elements to perturb\n",
    "grad_b1 = nn.grad_b1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_1[1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b1[1]\n",
    "print('Perturbing an element in b1. Ratio:', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "> Display  the  gradients  for  both  methods (direct computation and finite difference) for a small network (e.g. $d = 2$ and $d_{h} = 2$) with random weights and for a single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "> Add a hyperparameter for the minibatch size $K$ to allow computing the gradients on a minibatch of $K$ examples (in a matrix), by looping over the $K$ examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the above `neuralNet` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "> Display the gradients for both methods (direct computation and finite difference) for a small network (e.g. $d = 2$ and $d_{h} = 2$) with random weights and for a minibatch with 10 examples (you can use examples from both classes from the two circles dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of W2:\n",
      " [[-0.17793466 -0.27200777]\n",
      " [ 0.17793466  0.27200777]]\n",
      "Gradient of b2:\n",
      " [-0.50403059  0.50403059]\n",
      "Gradient of W1:\n",
      " [[-0.36610069  0.24727624]\n",
      " [ 0.25196422 -0.17018478]]\n",
      "Gradient of b1:\n",
      " [-0.44178644  0.30405399]\n",
      "\n",
      "Perturbing an element in W2. Ratio: [ 0.99999912  0.99999912  0.99999912 -0.98400742  0.99999912  0.99999912\n",
      "  0.99999912 -0.98400742  0.99999912  0.99999912]\n",
      "Perturbing an element in W2. Ratio: [ 0.99999866  0.99999866  0.99999866 -0.98400789  0.99999866  0.99999866\n",
      "  0.99999866 -0.98400789  0.99999866  0.99999866]\n",
      "Perturbing an element in W2. Ratio: [ 1.00000088  1.00000088  1.00000088 -0.98400567  1.00000088  1.00000088\n",
      "  1.00000088 -0.98400567  1.00000088  1.00000088]\n",
      "Perturbing an element in W2. Ratio: [ 1.00000134  1.00000134  1.00000134 -0.98400521  1.00000134  1.00000134\n",
      "  1.00000134 -0.98400521  1.00000134  1.00000134]\n",
      "Perturbing an element in b2. Ratio: [ 0.99999752  0.99999752  0.99999752 -0.98400903  0.99999752  0.99999752\n",
      "  0.99999752 -0.98400903  0.99999752  0.99999752]\n",
      "Perturbing an element in b2. Ratio: [ 1.00000248  1.00000248  1.00000248 -0.98400407  1.00000248  1.00000248\n",
      "  1.00000248 -0.98400407  1.00000248  1.00000248]\n",
      "Perturbing an element in W1. Ratio: [ 0.9999982   0.9999982   0.9999982  -0.98400835  0.9999982   0.9999982\n",
      "  0.9999982  -0.98400835  0.9999982   0.9999982 ]\n",
      "Perturbing an element in W1. Ratio: [ 1.00000122  1.00000122  1.00000122 -0.98400533  1.00000122  1.00000122\n",
      "  1.00000122 -0.98400533  1.00000122  1.00000122]\n",
      "Perturbing an element in W1. Ratio: [ 1.00000124  1.00000124  1.00000124 -0.98400531  1.00000124  1.00000124\n",
      "  1.00000124 -0.98400531  1.00000124  1.00000124]\n",
      "Perturbing an element in W1. Ratio: [ 0.99999916  0.99999916  0.99999916 -0.98400739  0.99999916  0.99999916\n",
      "  0.99999916 -0.98400739  0.99999916  0.99999916]\n",
      "Perturbing an element in b1. Ratio: [ 0.99999783  0.99999783  0.99999783 -0.98400872  0.99999783  0.99999783\n",
      "  0.99999783 -0.98400872  0.99999783  0.99999783]\n",
      "Perturbing an element in b1. Ratio: [ 1.0000015   1.0000015   1.0000015  -0.98400505  1.0000015   1.0000015\n",
      "  1.0000015  -0.98400505  1.0000015   1.0000015 ]\n"
     ]
    }
   ],
   "source": [
    "# Note to TAs: we attempted this with a loop, but deep/shallow copying issues got in the way.\n",
    "x_i = trainData[0:10]\n",
    "y_i = trainTarget[0:10].T\n",
    "sigma = 1e-5\n",
    "nn = neuralNet(2, 2, 2, 10)\n",
    "\n",
    "# Perturbing W2\n",
    "# 1 of 4 elements to perturb\n",
    "nn.gradDescentLoop(x_i, y_i, 1)\n",
    "print('Gradient of W2:\\n', nn.grad_W2)\n",
    "print('Gradient of b2:\\n', nn.grad_b2)\n",
    "print('Gradient of W1:\\n', nn.grad_W1)\n",
    "print('Gradient of b1:\\n', nn.grad_b1)\n",
    "print()\n",
    "\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[0][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][0]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# 2 of 4 elements to perturb\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[0][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][1]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# 3 of 4 elements to perturb\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[1][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[1][0]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# 4 of 4 elements to perturb\n",
    "grad_W2 = nn.grad_W2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[1][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[1][1]\n",
    "print('Perturbing an element in W2. Ratio:', ratio)\n",
    "\n",
    "# Perturbing b2\n",
    "# 1 of 2 elements to perturb\n",
    "grad_b2 = nn.grad_b2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_2[0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b2[0]\n",
    "print('Perturbing an element in b2. Ratio:', ratio)\n",
    "\n",
    "# 2 of 2 elements to perturb\n",
    "grad_b2 = nn.grad_b2\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_2[1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b2[1]\n",
    "print('Perturbing an element in b2. Ratio:', ratio)\n",
    "\n",
    "# Perturbing W1\n",
    "# 1 of 4 elements to perturb\n",
    "nn.gradDescentLoop(x_i, y_i, 1)\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[0][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[0][0]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# 2 of 4 elements to perturb\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[0][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[0][1]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# 3 of 4 elements to perturb\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[1][0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[1][0]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# 4 of 4 elements to perturb\n",
    "grad_W1 = nn.grad_W1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_1[1][1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W1[1][1]\n",
    "print('Perturbing an element in W1. Ratio:', ratio)\n",
    "\n",
    "# Perturbing b1\n",
    "# 1 of 2 elements to perturb\n",
    "grad_b1 = nn.grad_b1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_1[0] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b1[0]\n",
    "print('Perturbing an element in b1. Ratio:', ratio)\n",
    "\n",
    "# 2 of 2 elements to perturb\n",
    "grad_b1 = nn.grad_b1\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.b_1[1] += sigma\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_b1[1]\n",
    "print('Perturbing an element in b1. Ratio:', ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "> Train your neural network using gradient descent on the two circles dataset. Plot the decision regions for several different values of the hyperparameters (weight decay, number of hidden units, early stopping) so as to illustrate their effect on the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the circle training dataset\n",
    "plotTrainTarget = np.argmax(trainTarget, axis = 1)\n",
    "plt.scatter(trainData[:,0], trainData[:,1], c=plotTrainTarget, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(nn, data, target, K, num_epoch, fixed = False): \n",
    "    '''\n",
    "    train minibtaches over K epochs (in a loop)\n",
    "    also does prediction and error calcualation\n",
    "    '''\n",
    "    # Get minibatch\n",
    "    batchSampler = BatchSampler(data, target, K)\n",
    "    numBatch = data.shape[0] // K \n",
    "    # training loop\n",
    "    for n in range(num_epoch):\n",
    "        # Do descent and update params - this is one epoch\n",
    "        for i in range(numBatch):\n",
    "            if fixed:\n",
    "                batchData, batchTarget = batchSampler.get_batch(K)\n",
    "            elif not fixed:\n",
    "                batchData, batchTarget = batchSampler.get_batch()\n",
    "            #difference: another loop here\n",
    "            nn.gradDescentLoop(batchData, batchTarget.T, K)\n",
    "            nn.updateParams()\n",
    "        if n % 100 == 0:\n",
    "            nn.fpropLoop(data, data.shape[0]) \n",
    "            print(\"Cross-entropy loss at the end of epoch {}: {}\".format(n, nn.errorRate(target.T, mode = 'loop')))\n",
    "            print(\"classification error at the end of epoch {}: {}\".format(n,\n",
    "                                                    classErr(np.argmax(target, axis = 1), nn.predBatch)))        \n",
    "    \n",
    "    # finalized weights, need to fprop and get the error rate \n",
    "    # a for loop inside the prop for each elem\n",
    "    nn.fpropLoop(data, data.shape[0]) \n",
    "    print(\"End of train loop process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(nn, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h betweethem\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    \n",
    "    newData =np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    nn.fpropLoop(newData, newData.shape[0])\n",
    "    Z = nn.predBatch\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)\n",
    "    plt.show()\n",
    "    \n",
    "def plotDecision(nn, data):\n",
    "    nn.fpropLoop(data, data.shape[0])\n",
    "    plt.scatter(data[:,0], data[:,1], c = nn.predBatch)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying the number of hidden units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on circles, varying number of hidden units\n",
    "# K is Size of batches\n",
    "K = 10\n",
    "\n",
    "num_hidden = [2,50,100]\n",
    "for hidden in num_hidden:\n",
    "    circleNet = neuralNet(2, hidden, 2, K) \n",
    "    #train_loop(circleNet, trainData, trainTarget, K, 1)\n",
    "    train_matrix(circleNet, trainData, trainTarget, K, 1)\n",
    "    print('Plotting the decision boundary for number of hidden units = {}.'.format(hidden))\n",
    "    plot_decision_boundary(circleNet,trainData, np.argmax(trainTarget, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we vary the number of epochs (looking at the error on the validation set would allow us to \"early stop\" at the point where the validation error is at its lowest):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = [10, 100, 1000]    \n",
    "for n in num_epoch:\n",
    "    circleNet = neuralNet(2, 15, 2, trainData.shape[0] ) # K = 2\n",
    "    train_loop(circleNet, trainData, trainTarget, K, n)\n",
    "    plot_decision_boundary(circleNet,trainData, np.argmax(trainTarget, axis = 1))\n",
    "    print('Plotting the decision boundary for number of epochs = {}.'.format(n))\n",
    "    #plotDecision(circleNet,testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we vary the amount of regularization (weight decay):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No regularization\n",
    "circleNet = neuralNet(2, 15, 2, trainData.shape[0]) # K = 2\n",
    "train_loop(circleNet, trainData, trainTarget, K, 200) # 1000 epoches\n",
    "print('No regularization:')\n",
    "plot_decision_boundary(circleNet,trainData, np.argmax(trainTarget, axis = 1))\n",
    "\n",
    "\n",
    "# with regularization\n",
    "circleNet = neuralNet(2, 15, 2, trainData.shape[0], regularize=[3e-4, 3e-4,  3e-4, 3e-4]) # K = 2\n",
    "train_loop(circleNet, trainData, trainTarget, K, 200) # 1000 epoches\n",
    "print('Regularization: {}'.format(circleNet.regularize))\n",
    "plot_decision_boundary(circleNet,trainData, np.argmax(trainTarget, axis = 1))\n",
    "\n",
    "\n",
    "# with regularization\n",
    "circleNet = neuralNet(2, 15, 2, trainData.shape[0], regularize=[2e-3, 2e-3,  2e-3, 2e-3]) # K = 2\n",
    "train_loop(circleNet, trainData, trainTarget, K, 200) # 1000 epoches\n",
    "print('Regularization: {}'.format(circleNet.regularize))\n",
    "plot_decision_boundary(circleNet,trainData, np.argmax(trainTarget, axis = 1))\n",
    "\n",
    "\n",
    "\n",
    "# with different regularization\n",
    "circleNet = neuralNet(2, 15, 2, trainData.shape[0], regularize=[2e-2, 1e-2,  2e-2, 1e-2]) # K = 2\n",
    "train_loop(circleNet, trainData, trainTarget, K, 200) # 1000 epoches\n",
    "print('Regularization: {}'.format(circleNet.regularize))\n",
    "plot_decision_boundary(circleNet,trainData, np.argmax(trainTarget, axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "> As a second step, copy your existing implementation to modify it to a new implementation that will use matrix calculus (instead of a loop) on batches of size $K$ to improve efficiency. **Take the matrix expressions in numpy derived in the first part, and adapt them for a minibatch of size $K$. Show in your report what you have modified (describe the former and new expressions with the shapes of each matrix).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(See other notebook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "\n",
    "> Compare both implementations (with a loop and with matrix calculus) to check that they both give the same values for the gradients on the parameters, first for $K = 1$, then for $K = 10$. Display the gradients for both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the loop method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop with K = 1\n",
    "K = 1\n",
    "test_net = neuralNet(2, 5, 2, K)\n",
    "train_loop(test_net, trainData[0:1], trainTarget[0:1], K, 1, fixed = True)\n",
    "\n",
    "print('Gradients for K = 1 for loop method')\n",
    "print('grad_W1:', test_net.grad_W1)\n",
    "print('grad_b1:', test_net.grad_b1)\n",
    "print('grad_W2:', test_net.grad_W2)\n",
    "print('grad_b2:', test_net.grad_b2)\n",
    "\n",
    "print()\n",
    "\n",
    "# Loop with K = 10\n",
    "K = 10\n",
    "test_net = neuralNet(2, 5, 2, K, fixed=True)\n",
    "train_loop(test_net, trainData[0:10], trainTarget[0:10], K, 1, fixed = True)\n",
    "\n",
    "print('Gradients for K = 10 for matrix method')\n",
    "print('grad_W1:', test_net.grad_W1)\n",
    "print('grad_b1:', test_net.grad_b1)\n",
    "print('grad_W2:', test_net.grad_W2)\n",
    "print('grad_b2:', test_net.grad_b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8\n",
    "\n",
    "> Time how long an epoch takes on Fashion MNIST (1 epoch = 1 full traversal through the whole training set) for $K = 100$ for both versions (loop over a minibatch and matrix caluclus)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the loop method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the Fashion MINIST\n",
    "# K is batch size\n",
    "K = 100\n",
    "num_epochs = 1\n",
    "\n",
    "# Loop method\n",
    "fmNet = neuralNet(X_train.shape[1], 15, 10, K)\n",
    "start_time = time.time()\n",
    "train_loop(fmNet, X_train, y_train, K, num_epochs)\n",
    "print(\"--- 1 epoch on MNIST (loop method): {} seconds ---\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "\n",
    "> Adapt your code to compute the error (proportion of misclassified examples) on the training set as well as the total loss on the training set during each epoch of the training procedure, and at the end of each epoch, it computes the error and average loss on the validation set and the test set. Display the 6 corresponding figures (error and average loss on train/valid/test), and write them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10\n",
    "\n",
    "> Train your network on the Fashion MNIST dataset. Plot the training/valid/test curves (error and loss as a function of the epoch number, corresponding to what you wrote in a file in the last question). Add to your report the curves obtained using your best hyperparameters, i.e. for which you obtained your best error on the validations et. We suggest 2 plots: the first one will plot the error rate (train/valid/test with different colors, show which color in a legend) and the other one for the averaged loss (on train/valid/test). You should be able to get less than 20% test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
