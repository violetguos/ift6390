{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Part: Neural Network Implementation & Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Fashion MNIST data:\n",
    "Note: keep your file structures like this for reading input data without\n",
    "using ```import os``` for path change!\n",
    "```\n",
    "./Homework 3\n",
    "├── 3_practical_part.ipynb\n",
    "├── circles.txt\n",
    "├── data\n",
    "│   ├── fashion\n",
    "│   │   ├── t10k-images-idx3-ubyte.gz\n",
    "│   │   ├── t10k-labels-idx1-ubyte.gz\n",
    "│   │   ├── train-images-idx3-ubyte.gz\n",
    "│   │   └── train-labels-idx1-ubyte.gz\n",
    "│   └── mnist\n",
    "│       └── README.md\n",
    "├── hw3\n",
    "│   └── d3english.pdf\n",
    "├── overleaf_url.txt\n",
    "└── utils\n",
    "    ├── __init__.py\n",
    "    ├── __pycache__\n",
    "    │   ├── __init__.cpython-36.pyc\n",
    "    │   └── mnist_reader.cpython-36.pyc\n",
    "    ├── argparser.py\n",
    "    ├── helper.py\n",
    "    └── mnist_reader.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "import numpy as np\n",
    "\n",
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "\n",
    "print(X_test.shape)\n",
    "#os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Circles data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "circles_data = np.loadtxt(open('circles.txt','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (5000, 784) (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "class loadData:\n",
    "    def __init__(self):\n",
    "        self.flatten = True\n",
    "        self.addOnes = False\n",
    "        \n",
    "        self.data_path = '/data/'\n",
    "    def arrFlatten(self, arr):\n",
    "        '''\n",
    "        type np array\n",
    "        '''\n",
    "        dataDim1, dum1, dum2 = arr.shape\n",
    "        dum_sq = dum1 * dum2\n",
    "        arr = np.reshape(arr, [ dataDim1 ,dum_sq ])\n",
    "        return arr   \n",
    "    \n",
    "    def convertTarget(self, targetValues):\n",
    "        numClasses = np.max(targetValues) + 1\n",
    "        return np.eye(numClasses)[targetValues]\n",
    "    \n",
    "    def loadNumData(self):\n",
    "        X_train, y_train = mnist_reader.load_mnist('data/fashion', kind ='train')\n",
    "        X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "        \n",
    "        np.random.seed(6390)\n",
    "        randIndx = np.arange(len(X_test))\n",
    "        np.random.shuffle(randIndx)  \n",
    "        test_indices = np.random.permutation(X_test.shape[0])\n",
    "        test_data, test_labels = X_test[test_indices], y_test[test_indices]\n",
    "        \n",
    "        trainData, trainTarget = X_train, y_train\n",
    "        validData, validTarget = X_test[:5000], y_test[:5000]\n",
    "        testData, testTarget = X_test[5000:], y_test[5000:]\n",
    "            \n",
    "        trainTarget = self.convertTarget(trainTarget)\n",
    "        validTarget = self.convertTarget(validTarget)\n",
    "        testTarget = self.convertTarget(testTarget)\n",
    "\n",
    "\n",
    "        return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "dataLoader = loadData()\n",
    " \n",
    "trainData, trainTarget, validData, validTarget, testData, testTarget = dataLoader.loadNumData()\n",
    "print(trainData.shape, validData.shape, testData.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "> As a beginning, start with an implementation that computes the gradients for a single example, and check that the gradient is correct using the finite difference method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class neuralNet():\n",
    "    def __init__(self):\n",
    "        self.numberOfHidden\n",
    "        self.outputDim\n",
    "        self.inputDim\n",
    "        self.regularize # lambda value\n",
    "        \n",
    "        #may use xavier init\n",
    "        \n",
    "        # W_1 nc = data dim\n",
    "        self.W_1 = uniformSample[-1/sqrt(nc), 1/sqrt(nc)]\n",
    "        \n",
    "        # W_2 nc = hidden layer dim\n",
    "        self.W_2 = uniformSample[-1/sqrt(nc), 1/sqrt(nc)]\n",
    "        \n",
    "        self.b_1 = 0\n",
    "        self.b_2 = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def fprop():\n",
    "        h_a = np.dot(W_1, x) + self.b_1\n",
    "        h_s = relu(h_a)\n",
    "        o_a = np.dot(W_2, h_s) + self.b_2\n",
    "        o_s = softmax(o_a)\n",
    "    \n",
    "    def bprop():\n",
    "        grad_oa = \n",
    "        grad_b2 = \n",
    "        grad_W2 =\n",
    "        grad_hs =\n",
    "        grad_reg1 = # regularization for layer 1\n",
    "        grad_reg2 = # regularization for layer 2\n",
    "    \n",
    "    def gradDescentLoop():\n",
    "        miniBatch = sample(K indices)\n",
    "        for i in range(K):\n",
    "            bprop(a single scalar)\n",
    "    \n",
    "    def gradDescentMat():\n",
    "        miniBatch = sample(K indices)\n",
    "        #bprop using matrix operations\n",
    "        \n",
    "    \n",
    "    def errorRate():\n",
    "    \n",
    "    def dataSplit():\n",
    "        train\n",
    "        test\n",
    "        valid\n",
    "    \n",
    "\n",
    "def finiteGradCheck(net, sigma):\n",
    "    netCopy = net\n",
    "    netCopy.W_1 += sigma\n",
    "    # W_2 nc = hidden layer dim\n",
    "    netCopy.W_2 += sigma\n",
    "    netCopy.b_1 += sigma\n",
    "    netCopy.b_2 += sigma\n",
    "    \n",
    "    res = netCopy.fpop()\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "> Display  the  gradients  for  both  methods (direct computation and finite difference) for a small network (e.g. $d = 2$ and $d_{h} = 2$) with random weights and for a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "> Add a hyperparameter for the minibatch size $K$ to allow computing the gradients on a minibatch of $K$ examples (in a matrix), by looping over the $K$ examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "> Display the gradients for both methods (direct computation and finite difference) for a small network (e.g. $d = 2$ and $d_{h} = 2$) with random weights and for a minibatch with 10 examples (you can use examples from both classes from the two circles dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "> Train your neural network using gradient descent on the two circles dataset. Plot the decision regions for several different values of the hyperparameters (weight decay, number of hidden units, early stopping) so as to illustrate their effect on the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "> As a second step, copy your existing implementation to modify it to a new implementation that will use matrix calculus (instead of a loop) on batches of size $K$ to improve efficiency. **Take the matrix expressions in numpy derived in the first part, and adapt them for a minibatch of size $K$. Show in your report what you have modified (describe the former and new expressions with the shapes of each matrix).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "\n",
    "> Compare both implementations (with a loop and with matrix calculus) to check that they both give the same values for the gradients on the parameters, first for $K = 1$, then for $K = 10$. Display the gradients for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8\n",
    "\n",
    "> Time how long an epoch takes on Fashion MNIST (1 epoch = 1 full traversal through the whole training set) for $K = 100$ for both versions (loop over a minibatch and matrix caluclus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "\n",
    "> Adapt your code to compute the error (proportion of misclassified examples) on the training set as well as the total loss on the training set during each epoch of the training procedure, and at the end of each epoch, it computes the error and average loss on the validation set and the test set. Display the 6 corresponding figures (error and average loss on train/valid/test), and write them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10\n",
    "\n",
    "> Train your network on the Fashion MNIST dataset. Plot the training/valid/test curves (error and loss as a function of the epoch number, corresponding to what you wrote in a file in the last question). Add to your report the curves obtained using your best hyperparameters, i.e. for which you obtained your best error on the validations et. We suggest 2 plots: the first one will plot the error rate (train/valid/test with different colors, show which color in a legend) and the other one for the averaged loss (on train/valid/test). You should be able to get less than 20% test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
