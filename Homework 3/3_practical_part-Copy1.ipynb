{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Part: Neural Network Implementation & Experiments\n",
    "\n",
    "Team:\n",
    "* Jonathan Bhimani-Burrows ()\n",
    "* Arlie Coles (20121051)\n",
    "* Yue (Violet) Guo (20120727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Fashion MNIST data:\n",
    "Note: keep your file structures like this for reading input data without\n",
    "using ```import os``` for path change!\n",
    "```\n",
    "./Homework 3\n",
    "├── 3_practical_part.ipynb\n",
    "├── circles.txt\n",
    "├── data\n",
    "│   ├── fashion\n",
    "│   │   ├── t10k-images-idx3-ubyte.gz\n",
    "│   │   ├── t10k-labels-idx1-ubyte.gz\n",
    "│   │   ├── train-images-idx3-ubyte.gz\n",
    "│   │   └── train-labels-idx1-ubyte.gz\n",
    "│   └── mnist\n",
    "│       └── README.md\n",
    "├── hw3\n",
    "│   └── d3english.pdf\n",
    "├── overleaf_url.txt\n",
    "└── utils\n",
    "    ├── __init__.py\n",
    "    ├── __pycache__\n",
    "    │   ├── __init__.cpython-36.pyc\n",
    "    │   └── mnist_reader.cpython-36.pyc\n",
    "    ├── argparser.py\n",
    "    ├── helper.py\n",
    "    └── mnist_reader.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "import numpy as np\n",
    "import math\n",
    "import copy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/fashion\\\\train-labels-idx1-ubyte.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-1038c94ff2d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/fashion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_mnist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/fashion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m't10k'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\UdeM\\Automne 2018\\IFT 6390\\ift6390\\Homework 3\\utils\\mnist_reader.py\u001b[0m in \u001b[0;36mload_mnist\u001b[1;34m(path, kind)\u001b[0m\n\u001b[0;32m     12\u001b[0m                                % kind)\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlbpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n\u001b[0;32m     16\u001b[0m                                offset=8)\n",
      "\u001b[1;32mc:\\python37\\lib\\gzip.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mgz_mode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"t\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"write\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbinary_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGzipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgz_mode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\gzip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[0m\n\u001b[0;32m    161\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m             \u001b[0mfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmyfileobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/fashion\\\\train-labels-idx1-ubyte.gz'"
     ]
    }
   ],
   "source": [
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Circles data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "[[1 2]\n",
      " [2 3]\n",
      " [3 4]]\n",
      "(2, 3)\n",
      "1\n",
      "2\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "print(a.shape)\n",
    "c= np.stack((a, b))\n",
    "print(c.T)\n",
    "print(c.shape)\n",
    "\n",
    "print(c[0][0])\n",
    "print(c[1][0])\n",
    "print(c[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1100, 2)\n",
      "[1 1 0 ... 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "circlesData = np.loadtxt(open('circles.txt','r'))\n",
    "circlesTarget = circlesData[:,2]\n",
    "circlesData = circlesData[:,[0,1]] \n",
    "#circlesData = circlesData\n",
    "#circlesData = np.expand_dims(circlesData, axis = 1)\n",
    "#circlesData = circlesData.reshape(1100, 2, 1)\n",
    "print(circlesData.shape)\n",
    "circlesTarget = np.array([int(i) for i in circlesTarget])\n",
    "print(circlesTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 2) (110, 2) (110, 2)\n",
      "(880, 2)\n"
     ]
    }
   ],
   "source": [
    "class loadData:\n",
    "    def __init__(self):\n",
    "        self.addOnes = False\n",
    "        self.data_path = '/data/'\n",
    "    \n",
    "    def convertTarget(self, targetValues):\n",
    "        # Convert to one-hot encoding\n",
    "        numClasses = np.max(targetValues) + 1\n",
    "        return np.eye(numClasses)[targetValues]\n",
    "    \n",
    "\n",
    "    def loadNumData(self, data, target):\n",
    "        # Split into train/validation/test\n",
    "        np.random.seed(6390)\n",
    "        randIndices = np.random.permutation(data.shape[0])\n",
    "        data, target = data[randIndices], target[randIndices]\n",
    "        \n",
    "        div1 = int(math.floor(0.8 * data.shape[0]))\n",
    "        div2 = int(math.floor(0.9 * data.shape[0]))\n",
    "        trainData, trainTarget = data[:div1], target[:div1]\n",
    "        validData, validTarget = data[div1:div2], target[div1:div2]\n",
    "        testData, testTarget = data[div2:], target[div2:]\n",
    "    \n",
    "        # Get one hot encoding\n",
    "        trainTarget = self.convertTarget(trainTarget)\n",
    "        validTarget = self.convertTarget(validTarget)\n",
    "        testTarget = self.convertTarget(testTarget)\n",
    "        \n",
    "        return trainData, trainTarget, validData, validTarget, testData, testTarget\n",
    "\n",
    "dataLoader = loadData()\n",
    " \n",
    "trainData, trainTarget, validData, validTarget, testData, testTarget = dataLoader.loadNumData(circlesData, circlesTarget)\n",
    "print(trainData.shape, validData.shape, testData.shape)\n",
    "\n",
    "print(trainTarget.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "> As a beginning, start with an implementation that computes the gradients for a single example, and check that the gradient is correct using the finite difference method described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSampler(object):\n",
    "    '''\n",
    "    A (very) simple wrapper to randomly sample batches without replacement.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, targets, batch_size):\n",
    "        self.num_points = data.shape[0]\n",
    "        self.features = data.shape[1]\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "        self.batch_size = batch_size\n",
    "        self.indices = np.arange(self.num_points)\n",
    "\n",
    "    def random_batch_indices(self, m=None):\n",
    "        if m is None:\n",
    "            indices = np.random.choice(self.indices, self.batch_size, replace=False)\n",
    "        else:\n",
    "            indices = np.random.choice(self.indices, m, replace=False)\n",
    "        return indices \n",
    "\n",
    "    def get_batch(self, m=None):\n",
    "        '''\n",
    "        Get a random batch without replacement from the dataset.\n",
    "        If m is given the batch will be of size m. \n",
    "        Otherwise will default to the class initialized value.\n",
    "        '''\n",
    "        indices = self.random_batch_indices(m)\n",
    "        X_batch = np.take(self.data, indices, 0)\n",
    "        y_batch = self.targets[indices]\n",
    "        return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our own activation functions\n",
    "\n",
    "def relu(pre_activation):\n",
    "    '''\n",
    "    preactivation is a vector\n",
    "    '''\n",
    "    relu_output = np.zeros(pre_activation.shape)\n",
    "    relu_flat = relu_output.flatten()\n",
    "    for i, neuron in enumerate(pre_activation.flatten()):\n",
    "        if neuron > 0:\n",
    "            relu_flat[i] = neuron\n",
    "    relu_output = relu_flat.reshape(pre_activation.shape)\n",
    "    return relu_output\n",
    "\n",
    "def softmax(pre_activation):\n",
    "    '''\n",
    "    Numerically stable because subtracting the max value makes bit overflow impossible,\n",
    "    we will only have non-positive values in the vector\n",
    "    '''\n",
    "    exps = np.exp(pre_activation - np.max(pre_activation))\n",
    "    return exps / np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_fixed = np.array([[ 0.20960823 , 0.13663559], [ 0.38623373, -0.32807251] ,[-0.63849439 , 0.0131154 ],\n",
    "                     [ 0.5142807 ,  0.0595152 ], [-0.31075243 , 0.52335846]])\n",
    "w2_fixed = np.array( [[ 0.06159592, -0.10424877,  0.23591191 , 0.06177611 , 0.42799154],\n",
    "                      [ 0.40780062,  0.0759027  , 0.09284926, -0.14837115 ,  0.16844463]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      \\ndef earlyStopping(net):\\n    totalEpoch = 10 #may not be enough??\\n    for each epoch\\n        train\\n        valid\\n        test\\n        plot / print errors\\n'"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class neuralNet():\n",
    "    def __init__(self, d, dh, m, n, eta=1, regularize=None):\n",
    "        self.inputDim = d #inputDim\n",
    "        self.hiddenDim = dh #hiddenDim\n",
    "        self.outputDim = m #outputDim\n",
    "        self.regularize = regularize # lambda value\n",
    "        self.learningRate = eta\n",
    "        self.numData = n\n",
    "        self.batchErrorGradients = []\n",
    "        \n",
    "        #may use xavier init - maybe explore this later.\n",
    "        \n",
    "        # Initial weights and biases\n",
    "        self.W_1 = np.random.uniform(-1/np.sqrt(d), 1/np.sqrt(d), d*dh).reshape(dh, d)\n",
    "        #w1_fixed\n",
    "        \n",
    "        self.W_2 = np.random.uniform(-1/np.sqrt(dh), 1/np.sqrt(dh), dh*m).reshape(m, dh) \n",
    "        #w2_fixed\n",
    "        \n",
    "\n",
    "        self.b_1 = np.zeros(dh).reshape(dh,)\n",
    "        self.b_2 = np.zeros(m).reshape(m,)\n",
    "\n",
    "        \n",
    "    def fprop(self, batchData, batchTarget, mode='matrix'):\n",
    "        if mode == 'matrix':\n",
    "            stack_b1 = np.array([self.b_1,] * self.numData).T\n",
    "            self.h_a = np.dot(self.W_1, batchData.T) + stack_b1\n",
    "        elif mode == 'loop':\n",
    "            self.h_a = np.dot(self.W_1, batchData.T) + self.b_1\n",
    "            \n",
    "        #print(\"self.h_a \", self.h_a.shape)\n",
    "        # TODO: may come back to expand dim for single point\n",
    "        self.h_s = relu(self.h_a)\n",
    "        #print(\"h_s relu output \", self.h_s.shape)\n",
    "        \n",
    "        if mode == 'matrix':\n",
    "            stack_b2 = np.array([self.b_2,] * self.numData).T\n",
    "            self.o_a = np.dot(self.W_2, self.h_s) + stack_b2\n",
    "        elif mode == 'loop':\n",
    "            self.o_a = np.dot(self.W_2, self.h_s) + self.b_2\n",
    "            \n",
    "        #print(\"self oa \", self.o_a.shape)\n",
    "        self.o_s = softmax(self.o_a)\n",
    "        #print('output softmax o_s', self.o_s.shape)\n",
    "        \n",
    "\n",
    "    def errorRate(self, y, mode='matrix'):\n",
    "        '''\n",
    "        negative log\n",
    "        -logO_s(x)\n",
    "        '''        \n",
    "        \n",
    "        if mode == 'loop':\n",
    "            negLog = -self.o_a[np.argmax(y)] + np.log(np.sum(np.exp(self.o_a), axis=0))\n",
    "            \n",
    "        elif mode == 'matrix':\n",
    "            negLog = []\n",
    "            for i in range(self.numData):\n",
    "                error_at_point = -self.o_a[np.argmax(y[:,i])][i] + np.log(np.sum(np.exp(self.o_a), axis=0))[i]\n",
    "                negLog.append(error_at_point)\n",
    "            negLog = np.array(negLog)\n",
    "            negLog = np.mean(negLog)\n",
    "\n",
    "        return negLog\n",
    "          \n",
    "    def bpropLoop(self, batchData, batchTarget):\n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        self.grad_W2 = np.outer(self.grad_oa, self.h_s.T)\n",
    "        self.grad_b2 = self.grad_oa\n",
    "        self.grad_hs = np.dot(self.W_2.T , self.grad_oa)\n",
    "        h_a_stack = np.where(self.h_a > 0, 1, 0)\n",
    "        self.grad_ha = np.multiply(self.grad_hs, h_a_stack)\n",
    "        self.grad_W1 = np.outer(self.grad_ha, batchData)\n",
    "        self.grad_b1 = self.grad_ha\n",
    "        \n",
    "    def bprop(self, batchData, batchTarget, mode='matrix'):\n",
    "        '''\n",
    "        batchTarget already in one-hot format\n",
    "        '''\n",
    "        \n",
    "        #batch target must be m by n\n",
    "        #print('os', self.o_s.shape)\n",
    "        #self.grad_oa = np.mean((self.o_s - batchTarget), axis = 1)\n",
    "        self.grad_oa = self.o_s - batchTarget\n",
    "        #print('self.grad_oa', self.grad_oa)\n",
    "        #print('batchTarget', batchTarget)\n",
    "        #old_grad_oa = self.grad_oa\n",
    "    \n",
    "        #h_s_extend = np.repeat(self.h_s, self.outputDim ,axis = 0)  #[n*m, dh]\n",
    "        #h_s_extend = np.reshape(h_s_extend,[self.numData,  self.outputDim, self.hiddenDim]) #[n,m, dh]\n",
    "        #self.h_s = h_s_extend\n",
    "        #self.grad_oa = np.reshape(self.grad_oa, [self.numData, self.outputDim, 1])\n",
    "        \n",
    "        #print('self.grad_oa', self.grad_oa.shape)\n",
    "        \n",
    "        # 3D matmul??\n",
    "        #self.grad_W2 = np.multiply(self.grad_oa, self.h_s)\n",
    "        \n",
    "        \n",
    "        # debug outer prod averaging\n",
    "        i = 0\n",
    "        #print(\"self.grad_oa \\n\", self.grad_oa)\n",
    "        #print(\"self.grad_oa[:,i] \\n\", self.grad_oa[:,i])\n",
    "        \n",
    "        #print(\"self.h_s \\n\", self.h_s)\n",
    "        #print(\"self.h_s[:,i] \\n \", self.h_s[:,0])\n",
    "        \n",
    "        self.grad_W2 = [np.outer(self.grad_oa[:,i], self.h_s[:,i].T) for i in range(self.numData)]\n",
    "            \n",
    "        #print(\"all self.grad_W2 \", self.grad_W2)\n",
    "        self.grad_b2 = self.grad_oa #np.mean(self.grad_oa, axis = 1)\n",
    "        #self.grad_b2 = old_grad_oa\n",
    "        \n",
    "        self.grad_hs = np.dot(self.W_2.T , self.grad_oa)\n",
    "        \n",
    "        # Check this (dim mismatch maybe)\n",
    "        \n",
    "        #h_a_stack = np.where(self.h_a > 0, 1, 0).reshape(self.hiddenDim, self.numData, 1)\n",
    "        h_a_stack = np.where(self.h_a > 0, 1, 0)\n",
    "        \n",
    "        # dh * n * 1, stack to dh * n *d\n",
    "        #self.grad_ha = np.multiply(self.grad_hs , h_a_stack)\n",
    "        self.grad_ha = np.multiply(self.grad_hs, h_a_stack)\n",
    "        \n",
    "        \n",
    "        # grad ha = [dh, n]\n",
    "        #grad_ha_extend = np.repeat(self.grad_ha, self.numData, axis = 2) #[dh*d, n]\n",
    "        #print(\"grad_ha_extend \", grad_ha_extend.shape)\n",
    "        #self.grad_ha = grad_ha_extend\n",
    "        #batchData_extend = np.reshape(batchData, [self.numData, 1, self.inputDim])\n",
    "        #grad_ha_extend = np.reshape(self.grad_ha, [self.numData, self.hiddenDim, self.inputDim])\n",
    "        #self.grad_W1 = np.multiply(self.grad_ha, batchData_extend) #[n, d, dh]\n",
    "        self.grad_W1 = [np.outer(self.grad_ha[:,i], batchData[i]) for i in range(self.numData)]\n",
    "         \n",
    "        # temporary hack for grad_W1\n",
    "        \n",
    "        #print('grad_W1', self.grad_W1)\n",
    "        self.grad_b1 = self.grad_ha\n",
    "        #self.grad_b1 = self.grad_b1.T\n",
    "\n",
    "        #print('grad_b1', self.grad_b1)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def updateParams(self):\n",
    "        self.W_1 -= self.grad_W1 * self.learningRate\n",
    "        self.W_2 -= self.grad_W2 * self.learningRate\n",
    "        self.b_1 -= self.grad_b1 * self.learningRate\n",
    "        self.b_2 -= self.grad_b2 * self.learningRate\n",
    "    \n",
    "    \n",
    "    def gradDescentLoop(self, batchData, batchTarget, K):\n",
    "        # Call each example in the data (over the minibatches) in a loop\n",
    "        grad_W2, grad_b2, grad_W1, grad_b1 = [], [], [], []\n",
    "        for i in range(K):\n",
    "            self.fprop(batchData[i], batchTarget[:,i], mode='loop') #batchTarget[:,i]\n",
    "            self.bpropLoop(batchData[i],batchTarget[:,i])\n",
    "            grad_W2.append(self.grad_W2)\n",
    "            grad_b2.append(self.grad_b2)\n",
    "            grad_W1.append(self.grad_W1)\n",
    "            grad_b1.append(self.grad_b1)\n",
    "        self.grad_W2 = np.mean(grad_W2, axis=0)\n",
    "        self.grad_b2 = np.mean(grad_b2, axis=0)\n",
    "        self.grad_W1 = np.mean(grad_W1, axis=0)\n",
    "        self.grad_b1 = np.mean(grad_b1, axis=0)\n",
    "        # Update params\n",
    "        #self.updateParams()\n",
    "            \n",
    "            \n",
    "    def gradDescentMat(self, batchData, batchTarget):\n",
    "        # Feed the entire data matrix in as input\n",
    "        self.fprop(batchData, batchTarget)\n",
    "        self.bprop(batchData, batchTarget)\n",
    "\n",
    "        \n",
    "    def dataSplit(self):\n",
    "        '''\n",
    "        train\n",
    "        test\n",
    "        valid\n",
    "        '''\n",
    "            \n",
    "'''      \n",
    "def earlyStopping(net):\n",
    "    totalEpoch = 10 #may not be enough??\n",
    "    for each epoch\n",
    "        train\n",
    "        valid\n",
    "        test\n",
    "        plot / print errors\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_i [[1. 1.]\n",
      " [0. 0.]]\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "old old error: [0.71394038 0.71394038]\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "                      \n",
      "**********************\n",
      "oldErr\n",
      " [0.71394038 0.71394038]\n",
      "newErr\n",
      " [0.71393782 0.71393782]\n",
      "newErr - oldErr\n",
      " [-2.56687699e-06 -2.56687699e-06]\n",
      "estimate \n",
      " [-0.2566877 -0.2566877]\n",
      "old gradient \n",
      " -0.2568595498877779\n",
      "ratio \n",
      " [0.99933095 0.99933095]\n",
      "**********************\n",
      "                      \n"
     ]
    }
   ],
   "source": [
    "# Testing for loop minibatch\n",
    "x_i = trainData[0:2]\n",
    "y_i = trainTarget[0:2].T\n",
    "\n",
    "print(\"y_i\", y_i)\n",
    "nn = neuralNet(2, 5, 2, 2)\n",
    "# d = 2\n",
    "# dh = 5\n",
    "# m =2\n",
    "# n = 2\n",
    "\n",
    "\n",
    "###########################\n",
    "#\n",
    "# W2\n",
    "#\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "# Get gradients over minibatch\n",
    "nn.gradDescentLoop(x_i, y_i, 2)\n",
    "#nn2 = copy.deepcopy(nn)\n",
    "\n",
    "# Debug with finite difference\n",
    "grad_W2 = nn.grad_W2\n",
    "\n",
    "# Get old error\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "#oldErr = np.mean(np.array(oldErr))\n",
    "print('old old error:', oldErr)\n",
    "\n",
    "\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[0][1] += 1e-5\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 2)\n",
    "# Get new error\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "#newErr = np.mean(np.array(newErr))\n",
    "\n",
    "\n",
    "\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][1]\n",
    "\n",
    "\n",
    "print(\"                      \")\n",
    "print(\"**********************\")\n",
    "print('oldErr\\n', oldErr)\n",
    "print('newErr\\n', newErr)\n",
    "print('newErr - oldErr\\n', newErr-oldErr)\n",
    "print(\"estimate \\n\", estimate)\n",
    "print(\"old gradient \\n\", grad_W2[0][1])\n",
    "print(\"ratio \\n\", ratio )\n",
    "print(\"**********************\")\n",
    "print(\"                      \")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(nn, x_i, y_i, mode='matrix'):\n",
    "    # NOTE: for matrix, start taking average in the bprop (maybe)\n",
    "    nn\n",
    "    \n",
    "    sigma = 1e-5\n",
    "    \n",
    "    #Get old error\n",
    "    oldErr = []\n",
    "    for j in y_i.T:\n",
    "        oldErr.append(nn.errorRate(j, mode='loop'))\n",
    "    oldErr = np.array(oldErr)\n",
    "    print('oldErr:', oldErr)\n",
    "    \n",
    "    # Loop over all elements of all params\n",
    "    paramNames = ['W2', 'b2', 'W1', 'b1']\n",
    "    params = [nn.W_2, nn.b_2, nn.W_1, nn.b_1]\n",
    "    gradients = [nn.grad_W2, nn.grad_b2, nn.grad_W1, nn.grad_b1]\n",
    "    for i, param in enumerate(params):\n",
    "        print('paramname:', paramNames[i])\n",
    "        print('Param:', param)\n",
    "        print('Gradient at param:', gradients[i])\n",
    "        nnDebug = nn\n",
    "        flat_param = np.ndarray.flatten(param)\n",
    "        print('flat param:', flat_param)\n",
    "        print('params[i]', params[i])\n",
    "        for element in flat_param:\n",
    "            # Perturb one element\n",
    "            print('element:', element)\n",
    "            element += sigma\n",
    "            \n",
    "            # Get new error\n",
    "            if mode == 'matrix':\n",
    "                nnDebug.gradDescentMat(x_i, y_i)\n",
    "            elif mode == 'loop':\n",
    "                nnDebug.gradDescentLoop(x_i, y_i, 1)\n",
    "            newErr = []\n",
    "            for j in y_i.T:\n",
    "                newErr.append(nnDebug.errorRate(j, mode='loop'))\n",
    "            newErr = np.array(newErr)\n",
    "            print('newErr:', newErr)\n",
    "            \n",
    "            # The ratio of the estimate to the calculated gradient\n",
    "            # should be between 0.99 and 1.01.\n",
    "            estimate = (newErr - oldErr) / sigma\n",
    "            print('estimate:', estimate)\n",
    "            print('newErr - oldErr', newErr-oldErr)\n",
    "            print('element - sigma:', element-sigma)\n",
    "            print('np.where(params[i] == (element - sigma))', np.where(params[i] == (element - sigma)))\n",
    "            print('gradients[i][np.where(params[i] == (element - sigma))]', gradients[i][np.where(params[i] == (element - sigma))])\n",
    "            ratio = estimate / gradients[i][np.where(params[i] == (element - sigma))]\n",
    "            #gradients[i][np.where(params[i] == (element - sigma))]\n",
    "            print('Perturbing an element in {}, the ratio is: {}'.format(paramNames[i], ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_i [[1. 1.]\n",
      " [0. 0.]]\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "old old error: [0.71394038 0.71394038]\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "                      \n",
      "**********************\n",
      "oldErr\n",
      " [0.71394038 0.71394038]\n",
      "newErr\n",
      " [0.71393782 0.71393782]\n",
      "newErr - oldErr\n",
      " [-2.56687699e-06 -2.56687699e-06]\n",
      "estimate \n",
      " [-0.2566877 -0.2566877]\n",
      "old gradient \n",
      " -0.2568595498877779\n",
      "ratio \n",
      " [0.99933095 0.99933095]\n",
      "**********************\n",
      "                      \n",
      "oldErr: [0.71393782 0.71393782]\n",
      "paramname: W2\n",
      "Param: [[ 0.06159592 -0.10300877  0.23591191  0.06177611  0.42799154]\n",
      " [ 0.40780062  0.0759027   0.09284926 -0.14837115  0.16844463]]\n",
      "Gradient at param: [[-0.05021617 -0.25685892  0.         -0.20145047  0.        ]\n",
      " [ 0.05021617  0.25685892  0.          0.20145047  0.        ]]\n",
      "flat param: [ 0.06159592 -0.10300877  0.23591191  0.06177611  0.42799154  0.40780062\n",
      "  0.0759027   0.09284926 -0.14837115  0.16844463]\n",
      "params[i] [[ 0.06159592 -0.10300877  0.23591191  0.06177611  0.42799154]\n",
      " [ 0.40780062  0.0759027   0.09284926 -0.14837115  0.16844463]]\n",
      "element: 0.06159592\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.06159592\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.05021617]\n",
      "Perturbing an element in W2, the ratio is: [-13.58788693 -13.58788693]\n",
      "element: -0.10300877000000049\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: -0.10300877000000049\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.25685892]\n",
      "Perturbing an element in W2, the ratio is: [-2.65644537 -2.65644537]\n",
      "element: 0.23591191\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.23591191\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([2]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W2, the ratio is: [inf inf]\n",
      "element: 0.06177611\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.06177611\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([3]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.20145047]\n",
      "Perturbing an element in W2, the ratio is: [-3.38709406 -3.38709406]\n",
      "element: 0.42799154\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.42799154\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([4]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W2, the ratio is: [inf inf]\n",
      "element: 0.40780062\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.40780062\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.05021617]\n",
      "Perturbing an element in W2, the ratio is: [13.58788693 13.58788693]\n",
      "element: 0.0759027\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.0759027\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.25685892]\n",
      "Perturbing an element in W2, the ratio is: [2.65644537 2.65644537]\n",
      "element: 0.09284926\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.09284926\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([2]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W2, the ratio is: [inf inf]\n",
      "element: -0.14837115\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: -0.14837115\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([3]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.20145047]\n",
      "Perturbing an element in W2, the ratio is: [3.38709406 3.38709406]\n",
      "element: 0.16844463\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.16844463\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([4]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W2, the ratio is: [inf inf]\n",
      "paramname: b2\n",
      "Param: [0. 0.]\n",
      "Gradient at param: [-0.51028967  0.51028967]\n",
      "flat param: [0. 0.]\n",
      "params[i] [0. 0.]\n",
      "element: 0.0\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.0\n",
      "np.where(params[i] == (element - sigma)) (array([0, 1]),)\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.51028967  0.51028967]\n",
      "Perturbing an element in b2, the ratio is: [-1.33714578  1.33714578]\n",
      "element: 0.0\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.0\n",
      "np.where(params[i] == (element - sigma)) (array([0, 1]),)\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.51028967  0.51028967]\n",
      "Perturbing an element in b2, the ratio is: [-1.33714578  1.33714578]\n",
      "paramname: W1\n",
      "Param: [[ 0.20960823  0.13686559]\n",
      " [ 0.38623373 -0.32807251]\n",
      " [-0.63849439  0.0131154 ]\n",
      " [ 0.5142807   0.0595152 ]\n",
      " [-0.31075243  0.52335846]]\n",
      "Gradient at param: [[ 0.14695896 -0.09804314]\n",
      " [ 0.07594537 -0.05066668]\n",
      " [ 0.          0.        ]\n",
      " [-0.08920451  0.05951247]\n",
      " [ 0.          0.        ]]\n",
      "flat param: [ 0.20960823  0.13686559  0.38623373 -0.32807251 -0.63849439  0.0131154\n",
      "  0.5142807   0.0595152  -0.31075243  0.52335846]\n",
      "params[i] [[ 0.20960823  0.13686559]\n",
      " [ 0.38623373 -0.32807251]\n",
      " [-0.63849439  0.0131154 ]\n",
      " [ 0.5142807   0.0595152 ]\n",
      " [-0.31075243  0.52335846]]\n",
      "element: 0.20960823\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.20960823\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.14695896]\n",
      "Perturbing an element in W1, the ratio is: [4.64300845 4.64300845]\n",
      "element: 0.13686559000000023\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.13686559000000023\n",
      "np.where(params[i] == (element - sigma)) (array([0]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.09804314]\n",
      "Perturbing an element in W1, the ratio is: [-6.95950477 -6.95950477]\n",
      "element: 0.38623373\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.38623373\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.07594537]\n",
      "Perturbing an element in W1, the ratio is: [8.98450697 8.98450697]\n",
      "element: -0.32807251\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: -0.32807251\n",
      "np.where(params[i] == (element - sigma)) (array([1]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.05066668]\n",
      "Perturbing an element in W1, the ratio is: [-13.46706984 -13.46706984]\n",
      "element: -0.63849439\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: -0.63849439\n",
      "np.where(params[i] == (element - sigma)) (array([2]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W1, the ratio is: [inf inf]\n",
      "element: 0.0131154\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.0131154\n",
      "np.where(params[i] == (element - sigma)) (array([2]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W1, the ratio is: [inf inf]\n",
      "element: 0.5142807\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.5142807\n",
      "np.where(params[i] == (element - sigma)) (array([3]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [-0.08920451]\n",
      "Perturbing an element in W1, the ratio is: [-7.64907117 -7.64907117]\n",
      "element: 0.0595152\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.0595152\n",
      "np.where(params[i] == (element - sigma)) (array([3]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.05951247]\n",
      "Perturbing an element in W1, the ratio is: [11.46535654 11.46535654]\n",
      "element: -0.31075243\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: -0.31075243\n",
      "np.where(params[i] == (element - sigma)) (array([4]), array([0]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W1, the ratio is: [inf inf]\n",
      "element: 0.52335846\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.52335846\n",
      "np.where(params[i] == (element - sigma)) (array([4]), array([1]))\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [0.]\n",
      "Perturbing an element in W1, the ratio is: [inf inf]\n",
      "paramname: b1\n",
      "Param: [0. 0. 0. 0. 0.]\n",
      "Gradient at param: [ 0.17666468  0.09129668  0.         -0.10723598  0.        ]\n",
      "flat param: [0. 0. 0. 0. 0.]\n",
      "params[i] [0. 0. 0. 0. 0.]\n",
      "element: 0.0\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "newErr: [0.71394464 0.71394464]\n",
      "estimate: [0.68233168 0.68233168]\n",
      "newErr - oldErr [6.8233168e-06 6.8233168e-06]\n",
      "element - sigma: 0.0\n",
      "np.where(params[i] == (element - sigma)) (array([0, 1, 2, 3, 4]),)\n",
      "gradients[i][np.where(params[i] == (element - sigma))] [ 0.17666468  0.09129668  0.         -0.10723598  0.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikuo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:50: RuntimeWarning: divide by zero encountered in true_divide\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,) (5,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-642-5150ec5b902e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Debug with finite difference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnnDebug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;31m#get_old_error(nn, x_i, y_i, mode='loop')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-629-611cd1b76cdd>\u001b[0m in \u001b[0;36mdebug\u001b[0;34m(nn, x_i, y_i, mode)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'np.where(params[i] == (element - sigma))'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gradients[i][np.where(params[i] == (element - sigma))]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgradients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;31m#gradients[i][np.where(params[i] == (element - sigma))]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Perturbing an element in {}, the ratio is: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamNames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,) (5,) "
     ]
    }
   ],
   "source": [
    "# Testing for loop minibatch\n",
    "x_i = trainData[0:2]\n",
    "y_i = trainTarget[0:2].T\n",
    "\n",
    "print(\"y_i\", y_i)\n",
    "nn = neuralNet(2, 5, 2, 2)\n",
    "# d = 2\n",
    "# dh = 5\n",
    "# m =2\n",
    "# n = 2\n",
    "\n",
    "# Get gradients over minibatch\n",
    "nn.gradDescentLoop(x_i, y_i, 2)\n",
    "#nn2 = copy.deepcopy(nn)\n",
    "\n",
    "# Debug with finite difference\n",
    "grad_W2 = nn.grad_W2\n",
    "\n",
    "# Get old error\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "#oldErr = np.mean(np.array(oldErr))\n",
    "print('old old error:', oldErr)\n",
    "\n",
    "\n",
    "nnDebug = copy.deepcopy(nn)\n",
    "nnDebug.W_2[0][1] += 1e-5\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 2)\n",
    "\n",
    "# Get new error\n",
    "\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "#newErr = np.mean(np.array(newErr))\n",
    "\n",
    "\n",
    "\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][1]\n",
    "\n",
    "\n",
    "print(\"                      \")\n",
    "print(\"**********************\")\n",
    "print('oldErr\\n', oldErr)\n",
    "print('newErr\\n', newErr)\n",
    "print('newErr - oldErr\\n', newErr-oldErr)\n",
    "print(\"estimate \\n\", estimate)\n",
    "print(\"old gradient \\n\", grad_W2[0][1])\n",
    "print(\"ratio \\n\", ratio )\n",
    "print(\"**********************\")\n",
    "print(\"                      \")\n",
    "\n",
    "# Debug with finite difference\n",
    "debug(nnDebug, x_i, y_i, mode='loop')\n",
    "#get_old_error(nn, x_i, y_i, mode='loop')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "self.h_a  (5,)\n",
      "h_s relu output  (5,)\n",
      "self oa  (2,)\n",
      "output softmax o_s (2,)\n",
      "                      \n",
      "**********************\n",
      "estimate \n",
      " [-0.2566896 -0.2566896]\n",
      "old gradient \n",
      " -0.25686144936071936\n",
      "ratio \n",
      " [0.99933095 0.99933095]\n"
     ]
    }
   ],
   "source": [
    "# Debug with finite difference\n",
    "grad_W2 = nn.grad_W2\n",
    "\n",
    "# Get old error\n",
    "oldErr = []\n",
    "for i in y_i.T:\n",
    "    oldErr.append(nn.errorRate(i, mode='loop'))\n",
    "oldErr= np.array(oldErr)\n",
    "#oldErr = np.mean(np.array(oldErr))\n",
    "\n",
    "\n",
    "nnDebug = nn\n",
    "\n",
    "nnDebug.W_2[0][1] += 1e-5\n",
    "\n",
    "\n",
    "nnDebug.gradDescentLoop(x_i, y_i, 2)\n",
    "\n",
    "# Get new error\n",
    "\n",
    "newErr = []\n",
    "for i in y_i.T:\n",
    "    newErr.append(nnDebug.errorRate(i, mode='loop'))\n",
    "newErr = np.array(newErr)\n",
    "#newErr = np.mean(np.array(newErr))\n",
    "\n",
    "\n",
    "\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / grad_W2[0][1]\n",
    "\n",
    "\n",
    "print(\"                      \")\n",
    "print(\"**********************\")\n",
    "print(\"estimate \\n\", estimate)\n",
    "print(\"old gradient \\n\", grad_W2[0][1])\n",
    "print(\"ratio \\n\", ratio )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yi shape [[1. 1. 1.]\n",
      " [0. 0. 0.]]\n",
      "self.h_a  (5, 3)\n",
      "h_s relu output  (5, 3)\n",
      "self oa  (2, 3)\n",
      "output softmax o_s (2, 3)\n",
      "self.h_s \n",
      " [[0.09709241 0.09972198 0.        ]\n",
      " [0.50369354 0.50302453 0.3558222 ]\n",
      " [0.         0.         0.        ]\n",
      " [0.39286363 0.39668976 0.        ]\n",
      " [0.         0.         0.        ]]\n",
      "all self.grad_W2  [array([[-0.08122873, -0.42139637, -0.        , -0.32867467, -0.        ],\n",
      "       [ 0.01653276,  0.08576825,  0.        ,  0.06689629,  0.        ]]), array([[-0.08342105, -0.42079824, -0.        , -0.33184535, -0.        ],\n",
      "       [ 0.01698823,  0.08569322,  0.        ,  0.06757846,  0.        ]]), array([[-0.        , -0.29854955, -0.        , -0.        , -0.        ],\n",
      "       [ 0.        ,  0.0610433 ,  0.        ,  0.        ,  0.        ]])]\n",
      "grad_W1 [array([[ 0.01483989, -0.01002334],\n",
      "       [ 0.08231908, -0.05560097],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.06376472,  0.04306876],\n",
      "       [-0.        ,  0.        ]]), array([[ 0.01498368, -0.0098731 ],\n",
      "       [ 0.08294719, -0.05465584],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.06425821,  0.04234123],\n",
      "       [-0.        ,  0.        ]]), array([[ 0.        , -0.        ],\n",
      "       [ 0.0073954 , -0.09941039],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.        ,  0.        ]])]\n",
      "grad_b1 [[ 0.01790781  0.01794404  0.        ]\n",
      " [ 0.0993373   0.09933527  0.09968509]\n",
      " [-0.         -0.         -0.        ]\n",
      " [-0.07694711 -0.07695386 -0.        ]\n",
      " [-0.         -0.         -0.        ]]\n",
      " \n",
      "*******************\n",
      "grad w2\n",
      "[array([[-0.08122873, -0.42139637, -0.        , -0.32867467, -0.        ],\n",
      "       [ 0.01653276,  0.08576825,  0.        ,  0.06689629,  0.        ]]), array([[-0.08342105, -0.42079824, -0.        , -0.33184535, -0.        ],\n",
      "       [ 0.01698823,  0.08569322,  0.        ,  0.06757846,  0.        ]]), array([[-0.        , -0.29854955, -0.        , -0.        , -0.        ],\n",
      "       [ 0.        ,  0.0610433 ,  0.        ,  0.        ,  0.        ]])]\n",
      "grad W2 average\n",
      "[[-0.05488326 -0.38024805  0.         -0.22017334  0.        ]\n",
      " [ 0.01117367  0.07750159  0.          0.04482492  0.        ]]\n",
      "perturbed W2  -0.3802480532164541\n",
      " \n",
      "oldErr 0.7178539070938692\n",
      "self.h_a  (5, 3)\n",
      "h_s relu output  (5, 3)\n",
      "self oa  (2, 3)\n",
      "output softmax o_s (2, 3)\n",
      "self.h_s \n",
      " [[0.09709241 0.09972198 0.        ]\n",
      " [0.50369354 0.50302453 0.3558222 ]\n",
      " [0.         0.         0.        ]\n",
      " [0.39286363 0.39668976 0.        ]\n",
      " [0.         0.         0.        ]]\n",
      "all self.grad_W2  [array([[-0.08122869, -0.42139614, -0.        , -0.32867449, -0.        ],\n",
      "       [ 0.01653273,  0.08576806,  0.        ,  0.06689614,  0.        ]]), array([[-0.083421  , -0.42079801, -0.        , -0.33184517, -0.        ],\n",
      "       [ 0.01698819,  0.08569303,  0.        ,  0.06757831,  0.        ]]), array([[-0.        , -0.29854947, -0.        , -0.        , -0.        ],\n",
      "       [ 0.        ,  0.06104317,  0.        ,  0.        ,  0.        ]])]\n",
      "grad_W1 [array([[ 0.01483979, -0.01002327],\n",
      "       [ 0.08231208, -0.05559624],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.06376465,  0.04306871],\n",
      "       [-0.        ,  0.        ]]), array([[ 0.01498357, -0.00987303],\n",
      "       [ 0.08294014, -0.05465119],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.06425814,  0.04234118],\n",
      "       [-0.        ,  0.        ]]), array([[ 0.        , -0.        ],\n",
      "       [ 0.00739477, -0.09940197],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.        ,  0.        ],\n",
      "       [-0.        ,  0.        ]])]\n",
      "grad_b1 [[ 0.01790769  0.01794392  0.        ]\n",
      " [ 0.09932885  0.09932683  0.09967665]\n",
      " [-0.         -0.         -0.        ]\n",
      " [-0.07694703 -0.07695378 -0.        ]\n",
      " [-0.         -0.         -0.        ]]\n",
      "newErr 0.7178515826488377\n",
      "(newErr - oldErr) -2.3244450314541254e-06\n",
      "                      \n",
      "**********************\n",
      "estimate \n",
      " -0.2324445031454125\n",
      "old gradient \n",
      " -0.3802480532164541\n",
      "ratio \n",
      " 0.6112970235592363\n"
     ]
    }
   ],
   "source": [
    "x_i = trainData[0:3]\n",
    "y_i = trainTarget[0:3].T\n",
    "\n",
    "\n",
    "# !!! TODO: transpose all the targets in load data\n",
    "\n",
    "#print(\"x_i.shape\", x_i)\n",
    "print(\"yi shape\", y_i)\n",
    "nn = neuralNet(2, 5, 2, 3)\n",
    "\n",
    "# d = 2\n",
    "# dh = 5\n",
    "# m =2\n",
    "# n = 2\n",
    "\n",
    "nn.gradDescentMat(x_i, y_i)\n",
    "#nn.gradDescentLoop(x_i, y_i, 3)\n",
    "\n",
    "# a mat of weights for each \n",
    "grad_W2 = nn.grad_W2\n",
    "print(\" \")\n",
    "print(\"*******************\")\n",
    "print(\"grad w2\")\n",
    "print(grad_W2)\n",
    "\n",
    "print(\"grad W2 average\")\n",
    "avg_grad_w2 = np.mean(grad_W2, axis = 0)\n",
    "print(avg_grad_w2)\n",
    "print(\"perturbed W2 \", avg_grad_w2[0][1])\n",
    "print(\" \")\n",
    "oldErr = nn.errorRate(y_i)\n",
    "print('oldErr', oldErr)\n",
    "\n",
    "nnDebug = nn\n",
    "\n",
    "nnDebug.W_2[0][1] += 1e-5\n",
    "\n",
    "#nnDebug.fprop(x_i, y_i)\n",
    "nnDebug.gradDescentMat(x_i, y_i)\n",
    "newErr = nnDebug.errorRate(y_i)\n",
    "print('newErr', newErr)\n",
    "print('(newErr - oldErr)', (newErr - oldErr))\n",
    "\n",
    "\n",
    "estimate = (newErr - oldErr) / 1e-5\n",
    "ratio = estimate / avg_grad_w2[0][1]\n",
    "\n",
    "\n",
    "print(\"                      \")\n",
    "print(\"**********************\")\n",
    "print(\"estimate \\n\", estimate)\n",
    "print(\"old gradient \\n\", avg_grad_w2[0][1])\n",
    "print(\"ratio \\n\", ratio )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class debugNet():\n",
    "    \n",
    "    def __init__(self, originalNet):\n",
    "        self.copyNet = copy.deepcopy(originalNet)\n",
    "        self.originalNet = originalNet\n",
    "        self.idxList = ['W2', 'b2', 'W1', 'b1']\n",
    "        self.originalParamList = {'W2': originalNet.W_2, 'b2': originalNet.b_2, \n",
    "                             'W1': originalNet.W_1, 'b1': originalNet.b_1}\n",
    "        self.copyParamList = {'W2': self.copyNet.W_2, 'b2': self.copyNet.b_2, \n",
    "                              'W1': self.copyNet.W_1, 'b1': self.copyNet.b_1}\n",
    "        self.originalGradientList = {'W2': originalNet.grad_W2, 'b2': originalNet.grad_b2,\n",
    "                                    'W1': originalNet.grad_W1, 'b1': originalNet.grad_b1}\n",
    "        \n",
    "    def finiteGradCheck(self, sigma, x, y):\n",
    "        #for i, param in enumerate(copyParamList):\n",
    "        y = np.argmax(y)\n",
    "        print(\"y at top of debug \", y)\n",
    "\n",
    "        for key in self.idxList: #, param in self.copyParamList.items():\n",
    "            param = self.copyParamList[key]\n",
    "            print('key:', key, 'param:', param)\n",
    "            \n",
    "            '''if x.ndim == 1:\n",
    "                originalShape = self.originalParamList[key].shape\n",
    "                flatParams = np.ndarray.flatten(param)\n",
    "                print('flattened parameters:', flatParams)\n",
    "                for j, item in enumerate(flatParams):\n",
    "                    print('param j:', item)\n",
    "                    self.copyParamList[key][j] += sigma\n",
    "                    self.copyNet.fprop(x, y)\n",
    "                    originalScalar = self.originalParamList[key][j]\n",
    "                    grad_estimate = ( self.copyNet.errorRate(y) - self.originalNet.errorRate(y)) / sigma\n",
    "                    #print('self.originalGradientList', self.originalGradientList)\n",
    "                    print(\"self.originalGradientList[key] \", self.originalGradientList[key])\n",
    "                    originalGradient = self.originalGradientList[key][j]\n",
    "                    ratio = originalGradient / grad_estimate\n",
    "                    print('The ratio of calculated gradient and estimated gradient by perturbing {} gives {}.'.format(key, ratio))\n",
    "                    # Set back\n",
    "                    self.copyParamList[key][j] = originalScalar'''\n",
    "            if 1==1:\n",
    "                flatCopyParam = np.ndarray.flatten(param)\n",
    "                flatOriginalParam = np.ndarray.flatten(self.originalParamList[key])\n",
    "                originalShape = self.originalParamList[key].shape\n",
    "                for j, _ in enumerate(flatCopyParam):\n",
    "                    jRow = j % originalShape[0]\n",
    "                    jCol = j // originalShape[0]\n",
    "                    #print(\"original shape\", originalShape)\n",
    "                    #print(\"j % originalShape[0] = {}, j // originalShape[0] = {}\".format(j % originalShape[0],j // originalShape[0]))\n",
    "                    \n",
    "                    \n",
    "                    # Perturb\n",
    "                    if param.ndim > 1:\n",
    "                        self.copyParamList[key][jRow][jCol] += sigma\n",
    "                    else:\n",
    "                        self.originalParamList[key][jCol] +=sigma\n",
    "                    self.copyNet.fprop(x, y)\n",
    "                    \n",
    "                    if param.ndim > 1:\n",
    "                        originalScalar = self.originalParamList[key][jRow][jCol]\n",
    "                    else:\n",
    "                        originalScalar = self.originalParamList[key][jCol]\n",
    "                    grad_estimate = ( self.copyNet.errorRate(y) - self.originalNet.errorRate(y)) / sigma\n",
    "                    \n",
    "                    if param.ndim > 1:\n",
    "                        originalGradient = self.originalGradientList[key][jRow][jCol]\n",
    "                    else:\n",
    "                        originalGradient = self.originalGradientList[key][jCol]\n",
    "                    ratio = originalGradient / grad_estimate\n",
    "                    print('The ratio of calculated gradient and estimated gradient by perturbing {} gives \\\n",
    "                            {}.'.format(key, ratio))\n",
    "                    #print('original gradient', originalGradient)\n",
    "                    #print('grad_estimate ', grad_estimate)\n",
    "                    # Set back\n",
    "                    if param.ndim > 1:\n",
    "                        self.copyParamList[key][jRow][jCol] = originalScalar\n",
    "                    else:\n",
    "                        self.copyParamList[key][jCol] = originalScalar\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y.shape ()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-226-98991d1de0a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moldGradW_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_W2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moldErr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrorRate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oldGrad W2 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moldGradW_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-221-137a6a90e64a>\u001b[0m in \u001b[0;36merrorRate\u001b[0;34m(self, y, mode)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'matrix'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y.shape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mnegLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mo_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0mnegLog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegLog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#print('returning this error:', error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "sigma = 1e-5\n",
    "oldGradW_2 = nn.grad_W2\n",
    "oldErr = nn.errorRate(np.argmax(trainTarget[0]))\n",
    "print(\"oldGrad W2 \", oldGradW_2[0][0])\n",
    "nn.W_2[0] += sigma\n",
    "np.argmax(trainTarget[0])\n",
    "estimate = (nn.errorRate(np.argmax(trainTarget[0])) - oldErr) / sigma\n",
    "print(\"estimate \", estimate)\n",
    "print(\"ratio \", oldGradW_2[0][0] / estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  [ 0.82868251 -0.55971895]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-651-185c75741ccc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradDescentLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mtrainTarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'n'"
     ]
    }
   ],
   "source": [
    "# Do forward and backprop for one example\n",
    "\n",
    "# Initialize net\n",
    "print(\"input \", trainData[0:1][0])\n",
    "\n",
    "\n",
    "nn = neuralNet(2, 1, 2)\n",
    "nn.gradDescentLoop(trainData[0:1][0],  trainTarget[0], 1)\n",
    "\n",
    "# Print the gradients\n",
    "\n",
    "print('W1 gradient: \\n{} \\n\\n b1 gradient:\\n{}'.format(nn.grad_W1, nn.grad_b1))\n",
    "print(\"softmax result: \\n {}\".format(nn.o_s.shape))\n",
    "\n",
    "finiteGrad = debugNet(nn)\n",
    "\n",
    "# single input add one dim hack\n",
    "print(\"trainTarget[0:]\", trainTarget[0:1].T.shape)\n",
    "\n",
    "finiteGrad.finiteGradCheck(0.000001, trainData[0:1][0], trainTarget[0])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "> Display  the  gradients  for  both  methods (direct computation and finite difference) for a small network (e.g. $d = 2$ and $d_{h} = 2$) with random weights and for a single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3\n",
    "\n",
    "> Add a hyperparameter for the minibatch size $K$ to allow computing the gradients on a minibatch of $K$ examples (in a matrix), by looping over the $K$ examples (this is a small addition to your previous code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4\n",
    "\n",
    "> Display the gradients for both methods (direct computation and finite difference) for a small network (e.g. $d = 2$ and $d_{h} = 2$) with random weights and for a minibatch with 10 examples (you can use examples from both classes from the two circles dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "> Train your neural network using gradient descent on the two circles dataset. Plot the decision regions for several different values of the hyperparameters (weight decay, number of hidden units, early stopping) so as to illustrate their effect on the capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(nn, data, target, K, num_epoch): \n",
    "    # Get minibatch\n",
    "    batchSampler = BatchSampler(data, target, K)\n",
    "    numBatch = data.shape[0] // K \n",
    "    \n",
    "    for n in range(num_epoch):\n",
    "        # Do descent and update params - this is one epoch\n",
    "        for i in range(numBatch):\n",
    "            batchData, batchTarget = batchSampler.get_batch()\n",
    "            nn.gradDescentLoop(batchData, batchTarget, K)\n",
    "            nn.updateParams()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on circles\n",
    "K = 2\n",
    "\n",
    "num_hidden = [1,2,3,4]\n",
    "for hidden in num_hidden:\n",
    "    circleNet = neuralNet(2, hidden, 2, 20) # K = 2\n",
    "    train_loop(circleNet, trainData, trainTarget, K, 1)\n",
    "    \n",
    "weight_decay = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "for lamb in weight_decay: # Put this in nn class\n",
    "    circleNet = neuralNet(2, 2, 2, 20, weight_decay=lamb) # K = 2\n",
    "    train_loop(circleNet, trainData, trainTarget, K, 1, )\n",
    "    \n",
    "num_epoch = [1,2,3,4]\n",
    "for n in num_epoch:\n",
    "    train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6\n",
    "\n",
    "> As a second step, copy your existing implementation to modify it to a new implementation that will use matrix calculus (instead of a loop) on batches of size $K$ to improve efficiency. **Take the matrix expressions in numpy derived in the first part, and adapt them for a minibatch of size $K$. Show in your report what you have modified (describe the former and new expressions with the shapes of each matrix).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7\n",
    "\n",
    "> Compare both implementations (with a loop and with matrix calculus) to check that they both give the same values for the gradients on the parameters, first for $K = 1$, then for $K = 10$. Display the gradients for both methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 8\n",
    "\n",
    "> Time how long an epoch takes on Fashion MNIST (1 epoch = 1 full traversal through the whole training set) for $K = 100$ for both versions (loop over a minibatch and matrix caluclus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 9\n",
    "\n",
    "> Adapt your code to compute the error (proportion of misclassified examples) on the training set as well as the total loss on the training set during each epoch of the training procedure, and at the end of each epoch, it computes the error and average loss on the validation set and the test set. Display the 6 corresponding figures (error and average loss on train/valid/test), and write them in a log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 10\n",
    "\n",
    "> Train your network on the Fashion MNIST dataset. Plot the training/valid/test curves (error and loss as a function of the epoch number, corresponding to what you wrote in a file in the last question). Add to your report the curves obtained using your best hyperparameters, i.e. for which you obtained your best error on the validations et. We suggest 2 plots: the first one will plot the error rate (train/valid/test with different colors, show which color in a legend) and the other one for the averaged loss (on train/valid/test). You should be able to get less than 20% test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
