{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Practical Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def ridge_regression(weight, bias, weightedDecay, stepSize, numberSteps, data, target):\n",
    "    '''\n",
    "    weightedDecay: λ\n",
    "    stepSize: η\n",
    "    x: data\n",
    "    t: target\n",
    "    \n",
    "    '''\n",
    "    for i in range(0, numberSteps):\n",
    "        #gradient of a scalar x\n",
    "        grad = np.inverse(np.dot(data.T , data)) *data*(bias-target) \n",
    "        #regularization lambda\n",
    "        regGrad = weightedDecay * np.dot(data.T, data)\n",
    "        \n",
    "        regGrad = regGrad + grad\n",
    "        weight = weight - stepSize*regGrad\n",
    "    return w\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Draw Dn from h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([ 2.15970438, -2.38770133,  2.6684112 , -3.97092675, -4.52853786,\n",
      "        0.13972784, -3.63939382,  4.92175453,  1.63107077,  2.16477458,\n",
      "       -1.21866007, -1.59949983, -2.3325823 , -1.28242967,  2.10007598]), array([ 0.47945902, -2.40079123,  0.25624383, -1.45379622, -1.37541442,\n",
      "       -0.81880803, -1.61432342, -0.50163662,  0.48750528,  0.47815314,\n",
      "       -2.30423607, -2.47943803, -2.42337915, -2.34343855,  0.4931938 ])]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.uniform(-5,5,15)\n",
    "hX = np.sin(x) + 0.3*x -1\n",
    "dataDn = [x, hX]\n",
    "print(dataDn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Train without regularization, $\\lambda$ = 0\n",
    "Plot h(x), Dn, and our prediction function $f(X) = w^T X + b $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part 3 revisited, with difference lamda values\n",
    "Plot prediction with $W$ regularized by $\\lambda$ $\\lVert w \\rVert^2$\n",
    "Choose $\\lambda$ intermediate and large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample $D_{test}$ from $h(X)$\n",
    "Use $\\lambda$ = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "Plot $\\lambda$  on x-axis, avg((f(X) - target)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Use kernel $\\phi(x)$\n",
    "set $\\lambda = 0.01$\n",
    "\n",
    "Try different degrees of polynomials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comment on $l$, empirical risk ($D_n$), true risk($D_{test}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
